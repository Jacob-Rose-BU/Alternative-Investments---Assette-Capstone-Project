{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacob-Rose-BU/Alternative-Investments---Assette-Capstone-Project/blob/main/yfinance_Source.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "** Please note after conversations with the Business Advisor on Friday (prior to checkpoint submission), any sections talking about Holdings in this file is no longer valid. Updates will not offically been made to documentation until confirmation with the Business Advisor at the next meeting. Holdings will be created in the Snowflake database. Review Holdings File for SQL code (Fund Creation & Holding Allocation Pipeline)."
      ],
      "metadata": {
        "id": "UoTiRUUp2hgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ESG Equity Fact Sheet - YFinance Data Pipeline**"
      ],
      "metadata": {
        "id": "HMv5CCePsx5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook pulls real financial data using the Yahoo Finance API via the yfinance library. It generates tables for Security Master, ESG data, and historical price performance for U.S. equities.The extracted data is prepared to be loaded into Snowflake for downstream use in ESG fund fact sheets.\n",
        "\n",
        "\n",
        "###**Execution Instructions**\n",
        "\n",
        "**To run this notebook:**\n",
        "1. Update your Snowflake credentials in the environment or connection file.\n",
        "2. Run the notebook sequentially from top to bottom.\n",
        "\n",
        "### **File Roadmap**\n",
        "Pull S&P 500, NASDAQ 100, Dow Jones tickers <br>\n",
        "Pull yfinance data for valid tickers <br>\n",
        "Extract and clean ESG and performance history <br>\n",
        "Push to Snowflake\n",
        "\n",
        "**Output:** 3 tables in snowflake (security_master, esg_stock_data, stock_performance_history)\n",
        "\n",
        "\n",
        "### **Next Steps**\n",
        "#### **yfinance**\n",
        "- Improve ESG completeness check (what to do when ESG data is not given in yfinance - maybe pull in ESG API)\n",
        "- Add performance benchmark (SPESG & SUSL)\n",
        "\n",
        "####**Snowflake SQl Documentation**\n",
        "- Write code for Holdings creation in Snowflake (Friday Conversation w/ Corey)\n",
        "- Create documentation for reusable steps for top 10 holdings by weight\n",
        "- Create documentation for reusable steps for fund level ESG score aggregation\n",
        "- Create documentation for reusable steps for fund performance versus benchmark\n",
        "\n",
        "### **Future Improvement:**\n",
        "- Automate periodic data refresh\n",
        "- Add additional tickers\n",
        "- Backfull daily performance"
      ],
      "metadata": {
        "id": "KzrnrOqKs4X5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AyWxytZRHOp"
      },
      "source": [
        "# **Connect to Snowflake**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load data into Snowflake, we established a secure connection using credentials stored in a .env file. This connection allows us to push data directly from Python. The pipeline is designed to check if tables already exist, create them if needed, and merge new data while avoiding duplicates. This setup enables seamless integration between our local data processing and Snowflake's cloud warehouse, supporting scalable, centralized storage for downstream analytics like ESG reporting and fact sheet generation."
      ],
      "metadata": {
        "id": "HoH69l85ln6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "KGL2g66CSJpI",
        "outputId": "3068c300-8a53-4261-bb04-d01bb98cd6a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-891bad59-9157-4507-81b8-1bb56cbbe375\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-891bad59-9157-4507-81b8-1bb56cbbe375\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env.txt to .env.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.env.txt': b'SNOWFLAKE_ACCOUNT=assette-ssappoc\\nSNOWFLAKE_USER=CRYSTALL\\nSNOWFLAKE_PASSWORD=Bbnmghjtyu123!\\nSNOWFLAKE_ROLE=AST_ALTERNATIVES_DB_RW\\nSNOWFLAKE_WAREHOUSE=AST_BU_WH\\nSNOWFLAKE_DATABASE=AST_ALTERNATIVES_DB\\nSNOWFLAKE_SCHEMA=DBO'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#load the .env file\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi-7qVgwAzeB",
        "outputId": "a5ea4669-10f8-469c-f8cb-6f76ff2b0bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed .env.txt to .env\n"
          ]
        }
      ],
      "source": [
        "#rename the file if needed\n",
        "import os\n",
        "\n",
        "if os.path.exists(\".env.txt\"):\n",
        "    os.rename(\".env.txt\", \".env\")\n",
        "    print(\"Renamed .env.txt to .env\")\n",
        "else:\n",
        "    print(\"File not found. Make sure you uploaded .env.txt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEenJNbQA1Iz",
        "outputId": "60e3080d-9fe9-41f5-cfad-e256f27095b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
            "  Downloading boto3-1.39.15-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
            "  Downloading botocore-1.39.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.14.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.8)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (0.13.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
            "Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.39.15-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.39.15-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, python-dotenv, jmespath, botocore, s3transfer, boto3, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 boto3-1.39.15 botocore-1.39.15 jmespath-1.0.1 python-dotenv-1.1.1 s3transfer-0.13.1 snowflake-connector-python-3.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install snowflake-connector-python python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUlGi_CSA2yw",
        "outputId": "ce666c11-ed25-4221-c9f5-a18889fbf541"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import snowflake.connector\n",
        "\n",
        "# Load .env file data\n",
        "load_dotenv(\".env\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2AuY5_nkA512"
      },
      "outputs": [],
      "source": [
        "#use .env paramaters to connect to snowflake\n",
        "def get_snowflake_connection():\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.getenv(\"SNOWFLAKE_USER\"),\n",
        "        password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "        account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "        role=os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "        warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "        database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "        schema=os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
        "    )\n",
        "#connection - connection is authenticated\n",
        "connection = get_snowflake_connection()\n",
        "#lets me run SQL commands\n",
        "cursor = connection.cursor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PyoGZIlcDlwY"
      },
      "outputs": [],
      "source": [
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "\n",
        "def safe_quote(col: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensures column names are safely quoted for Snowflake SQL syntax.\n",
        "    Replaces internal quotes and wraps the name in double quotes.\n",
        "    \"\"\"\n",
        "    col = str(col).replace('\"', '\"\"').strip()\n",
        "    return f'\"{col}\"'\n",
        "\n",
        "def map_dtype_to_snowflake(dtype):\n",
        "    \"\"\"\n",
        "    Maps pandas dtypes to Snowflake SQL data types.\n",
        "    \"\"\"\n",
        "    if pd.api.types.is_float_dtype(dtype):\n",
        "        return \"FLOAT\"\n",
        "    elif pd.api.types.is_integer_dtype(dtype):\n",
        "        return \"NUMBER\"\n",
        "    else:\n",
        "        return \"VARCHAR\"\n",
        "\n",
        "def load_to_snowflake_merge(df, table_name, conn, unique_keys):\n",
        "    \"\"\"\n",
        "    Uploads DataFrame to Snowflake with type inference and merge logic.\n",
        "    \"\"\"\n",
        "    cur = conn.cursor()\n",
        "    df_cols = df.columns.tolist()\n",
        "    temp_table = f\"{table_name}_STAGING\"\n",
        "\n",
        "    # Step 1: Infer column types and create table if needed\n",
        "    col_defs = \", \".join([\n",
        "        f\"{safe_quote(col)} {map_dtype_to_snowflake(df[col].dtype)}\"\n",
        "        for col in df_cols\n",
        "    ])\n",
        "    cur.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} ({col_defs})\")\n",
        "\n",
        "    # Step 2: Add any missing columns to the main table\n",
        "    cur.execute(f\"DESC TABLE {table_name}\")\n",
        "    existing_cols = {row[0].upper() for row in cur.fetchall()}\n",
        "    for col in df_cols:\n",
        "        if col.upper() not in existing_cols:\n",
        "            col_type = map_dtype_to_snowflake(df[col].dtype)\n",
        "            cur.execute(f\"ALTER TABLE {table_name} ADD COLUMN {safe_quote(col)} {col_type}\")\n",
        "\n",
        "    # Step 3: Create staging table\n",
        "    cur.execute(f\"CREATE OR REPLACE TABLE {temp_table} ({col_defs})\")\n",
        "    write_pandas(conn, df, temp_table)\n",
        "\n",
        "    # Step 4: Merge without duplication\n",
        "    on_clause = \" AND \".join([f\"t.{safe_quote(col)} = s.{safe_quote(col)}\" for col in unique_keys])\n",
        "    insert_cols = \", \".join([safe_quote(col) for col in df_cols])\n",
        "    insert_vals = \", \".join([f\"s.{safe_quote(col)}\" for col in df_cols])\n",
        "\n",
        "    merge_stmt = f\"\"\"\n",
        "        MERGE INTO {table_name} t\n",
        "        USING {temp_table} s\n",
        "        ON {on_clause}\n",
        "        WHEN NOT MATCHED THEN\n",
        "            INSERT ({insert_cols}) VALUES ({insert_vals})\n",
        "    \"\"\"\n",
        "    cur.execute(merge_stmt)\n",
        "\n",
        "    # Step 5: Clean up\n",
        "    cur.execute(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"{table_name} updated. Duplicates prevented using keys: {unique_keys}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktPolCY7A8Nq",
        "outputId": "55eabae0-49bc-4d32-a09a-ad98128f4c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('CRYSTALL', 'AST_ALTERNATIVES_DB_RW', 'AST_ALTERNATIVES_DB', datetime.date(2025, 7, 29))\n"
          ]
        }
      ],
      "source": [
        "#test the connection\n",
        "cursor.execute(\"SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_DATE;\")\n",
        "\n",
        "for row in cursor:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1RCeSYqBDAZQ"
      },
      "outputs": [],
      "source": [
        "#close SQL cursor\n",
        "cursor.close()\n",
        "#close connection to snowflake\n",
        "connection.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Securities List**"
      ],
      "metadata": {
        "id": "ZhbkQe_IKVvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a clean and verified list of stocks by scraping 3 major US equity indices from Wikipedia: the S&P500, Dow Jones Industrial Average, and NASDAQ 100. Each of these index lists are retried through BeautifulSoup. The extracted tickers are combined into a single list, cleaned, conform to the expected format, and deduplicated between the 3 indexes. To ensure only valid tickers are included a function was defined to check that Yahoo Finance returns metadata.\n",
        "<br> <br>\n",
        "The tickers chosen (S&P500, Dow Jones Industrial Average, NASDAQ 100) represent large, liquid, and well known US companies. They are likely to be included in popular retail and institutional funds, making them a reasonable starting point for building fund simulations and a security master. The decision to limit the scope to these indives was intentional, by focusing on high confdence symbols, the code minimizes errors and avoids excessive querying that could trigger rate limits or bands fron the Yahoo Finance API."
      ],
      "metadata": {
        "id": "5TyqHAdOZ1uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "#get tickers from sp500\n",
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
        "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
        "    return [row.find_all(\"td\")[0].text.strip() for row in table.find_all(\"tr\")[1:]]\n",
        "\n",
        "#get tickers from dow jones indstrial\n",
        "def get_dow_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average\"\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
        "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
        "    return [\n",
        "        row.find_all(\"td\")[1].find(\"a\").text.strip()\n",
        "        for row in table.find_all(\"tr\")[1:]\n",
        "        if len(row.find_all(\"td\")) >= 2\n",
        "    ]\n",
        "\n",
        "#get tickers from NASDAQ-100\n",
        "def get_nasdaq100_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
        "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
        "    return [\n",
        "        row.find_all(\"td\")[0].text.strip()\n",
        "        for row in table.find_all(\"tr\")[1:]\n",
        "        if len(row.find_all(\"td\")) >= 1\n",
        "    ]\n",
        "\n",
        "\n",
        "#validate that these are real tickers\n",
        "def is_valid_ticker(ticker):\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return \"shortName\" in info\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "#pull all tickers from all of the above sources\n",
        "sp500 = get_sp500_tickers()\n",
        "dow = get_dow_tickers()\n",
        "nasdaq = get_nasdaq100_tickers()\n",
        "\n",
        "#combine and make tickers unique\n",
        "all_tickers = sorted(set(sp500 + dow + nasdaq))\n",
        "\n",
        "#data cleaning\n",
        "all_tickers = [t.replace('.', '-') for t in all_tickers]\n"
      ],
      "metadata": {
        "id": "Op7Ze24HKVJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Insert yFinance Data into Snowflake Tables**\n"
      ],
      "metadata": {
        "id": "ayhdSdLxr5Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed to build 3 key financial datasets, a security master table, ESG scores, and 10 years of historical price performance, for a list of securities. Using the Yahoo Finance API, the script loops through each ticker and retrieves metadata like company name, sector,, industry, market cap, and trading exchanges. It also attemps to fetch ESG related metrics (if available) and daily price history over the past 10 years. All this information is stored in separate DataFrames. If a ticker fails to return valid metadata or historical price data, it is logged into a failed ticker list. Once the data is collected, the script standardizes data formatting and pushes each DataFrame directly to Snowflake using a merg strategy, avoiding duplicates based on defined unique keys. <br> <br>\n",
        "Initally, the code included additional tables such as price snapshots, fundamentals, and analyst estimates. But, we decided to remove these from the pipeline because they aren't directly used in our target deliverable, the fund fact sheet. While they may be used in the broader portfolio analytics or internal risk assessments, they were out of scope for this specific task. <br>\n",
        "The security master table is foundational to the pipeline, as it centralizes all core attributes about the securities in our dataset. It ensures consistency and enables future joins with holdings, ESG metrics, and price data. We also collect daily price history over a 10 year horizon to support fund level performance analysis, invluding quarter over quarter or year over year changes. Ideally, this time range should be extended further to reflect real world investment horizons more accurately. However, we limited the query to 10 years to avoid triggering API rate limits of blocks from Yahoo Finance. A long-term enhancement would be periodically refresh historical data and biild a more robust price history system over time. <br> <br>\n",
        "While this current version is designed to run once and populate Snowflake, future iterations could introduce automated checks. Fo rexample, if a security appears in holdings but is missing from the security master, the system should automatically fetch and populate its metadata from Yahoo Finance. This would ensure the pipeline remains dynamic and scalable as fund compositions evolve."
      ],
      "metadata": {
        "id": "CRmKH6HuYJgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Initialize tables\n",
        "security_master = []\n",
        "esg = []\n",
        "performance = []\n",
        "failed_tickers = []\n",
        "\n",
        "# --- Pull 10 years of historical price data for each ticker ---\n",
        "start_date = (datetime.datetime.today() - datetime.timedelta(days=365 * 10)).strftime('%Y-%m-%d')\n",
        "end_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "# Replace this with your real ticker list\n",
        "# all_tickers = ['AAPL', 'TSLA', 'GOOG', 'INVALID1', 'INVALID2']\n",
        "# Assume all_tickers is defined above\n",
        "\n",
        "for symbol in all_tickers:\n",
        "    try:\n",
        "        t = yf.Ticker(symbol)\n",
        "        info = t.info\n",
        "\n",
        "        # Validate info is usable\n",
        "        # Skip invalid tickers that have no 'shortName' (likely delisted or bad symbol)\n",
        "\n",
        "        if not info or \"shortName\" not in info:\n",
        "            print(f\"No valid info for {symbol}\")\n",
        "            failed_tickers.append(symbol)\n",
        "            continue\n",
        "\n",
        "        # --- Security Master ---\n",
        "        security_master.append({\n",
        "            \"symbol\": symbol,\n",
        "            \"shortName\": info.get(\"shortName\"),\n",
        "            \"name\": info.get(\"longName\"),\n",
        "            \"sector\": info.get(\"sector\"),\n",
        "            \"industry\": info.get(\"industry\"),\n",
        "            \"exchange\": info.get(\"exchange\"),\n",
        "            \"currency\": info.get(\"currency\"),\n",
        "            \"country\": info.get(\"country\"),\n",
        "            \"market_cap\": info.get(\"marketCap\")\n",
        "        })\n",
        "\n",
        "        # --- ESG Data ---\n",
        "        sustainability = t.sustainability\n",
        "        if sustainability is not None and not sustainability.empty:\n",
        "            row = sustainability.transpose()\n",
        "            esg.append({\n",
        "                \"symbol\": symbol,\n",
        "                \"esgPerformance\": row.get(\"esgPerformance\", {}).values[0] if \"esgPerformance\" in row else None,\n",
        "                \"totalEsg\": row.get(\"totalEsg\", {}).values[0] if \"totalEsg\" in row else None,\n",
        "                \"environmentScore\": row.get(\"environmentScore\", {}).values[0] if \"environmentScore\" in row else None,\n",
        "                \"socialScore\": row.get(\"socialScore\", {}).values[0] if \"socialScore\" in row else None,\n",
        "                \"governanceScore\": row.get(\"governanceScore\", {}).values[0] if \"governanceScore\" in row else None,\n",
        "                \"highestControversy\": row.get(\"highestControversy\", {}).values[0] if \"highestControversy\" in row else None\n",
        "            })\n",
        "\n",
        "        # --- Performance History ---\n",
        "        hist = t.history(start=start_date, end=end_date)\n",
        "        if hist.empty:\n",
        "            print(f\"No price history for {symbol}\")\n",
        "            failed_tickers.append(symbol)\n",
        "            continue\n",
        "        hist = hist.reset_index()\n",
        "        hist[\"symbol\"] = symbol\n",
        "        performance.append(hist)\n",
        "\n",
        "        # Optional: sleep to avoid hitting rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {symbol}: {e}\")\n",
        "        failed_tickers.append(symbol)\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_security_master = pd.DataFrame(security_master)\n",
        "df_esg = pd.DataFrame(esg)\n",
        "df_performance = pd.concat(performance, ignore_index=True) if performance else pd.DataFrame()\n",
        "\n",
        "# Output summary\n",
        "print(f\"\\nFinished processing {len(all_tickers)} tickers.\")\n",
        "print(f\"Total failed tickers: {len(failed_tickers)}\")\n",
        "print(failed_tickers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZpcgw9Vr7fJ",
        "outputId": "dd2213d9-7d76-4014-fd9f-02bd8505fa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished processing 517 tickers.\n",
            "Total failed tickers: 0\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert table into a Dataframe\n",
        "df_security_master = pd.DataFrame(security_master)\n",
        "df_esg = pd.DataFrame(esg)\n",
        "df_performance = pd.concat(performance) if performance else pd.DataFrame()\n",
        "\n",
        "df_performance[\"Date\"] = pd.to_datetime(df_performance[\"Date\"], unit='ns').dt.date\n"
      ],
      "metadata": {
        "id": "oaNkrtcdsAaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to snowflake and load the data directly bypassing any previously loaded data\n",
        "conn = get_snowflake_connection()\n",
        "load_to_snowflake_merge(df_security_master, \"SECURITY_MASTER\", conn, unique_keys=[\"symbol\"])\n",
        "\n",
        "conn = get_snowflake_connection()\n",
        "load_to_snowflake_merge(df_esg, \"ESG_STOCK_DATA\", conn, unique_keys=[\"symbol\"])\n",
        "\n",
        "conn = get_snowflake_connection()\n",
        "if not df_performance.empty:\n",
        "    load_to_snowflake_merge(df_performance, \"STOCK_PERFORMANCE_HISTORY\", conn, unique_keys=[\"Date\", \"symbol\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxYngE3btHjM",
        "outputId": "93256936-f784-4b38-a2cf-e7a208407750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SECURITY_MASTER updated. Duplicates prevented using keys: ['symbol']\n",
            "ESG_STOCK_DATA updated. Duplicates prevented using keys: ['symbol']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-41-2320129031.py:47: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
            "  write_pandas(conn, df, temp_table)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STOCK_PERFORMANCE_HISTORY updated. Duplicates prevented using keys: ['Date', 'symbol']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_esg.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kis383Q53S8k",
        "outputId": "e8728d39-48a6-4707-a721-d5d91a21d722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 511 entries, 0 to 510\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   symbol              511 non-null    object \n",
            " 1   esgPerformance      511 non-null    object \n",
            " 2   totalEsg            511 non-null    float64\n",
            " 3   environmentScore    509 non-null    float64\n",
            " 4   socialScore         509 non-null    float64\n",
            " 5   governanceScore     509 non-null    float64\n",
            " 6   highestControversy  511 non-null    float64\n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 28.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# yFinance Benchmark Indexes"
      ],
      "metadata": {
        "id": "xtfCqfKZ2yOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "theres no esg score for indexes\n",
        "chose one index for each fund focus.\n",
        "initally hardcoded some column fields but in order to make it repeatable and applied to other areas then did a config. also maybe reference currency info from the currency table instead of currency_full_name.\n",
        "\n",
        "add this information into the 2-3 tables that are available for benchmarks.\n",
        "\n",
        "make its own table in snowflake. do a join between benchmark tables and"
      ],
      "metadata": {
        "id": "NJtQc4DDTsCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------- CONFIG -------------------\n",
        "#update as benchmarks are chosen, added on\n",
        "benchmark_map = {\n",
        "    \"ENRG\": \"Environment_ENRG\",\n",
        "    \"SHE\":  \"Social_SHE\",\n",
        "    \"VOTE\": \"Governance_VOTE\",\n",
        "    \"ESGD\": \"LowControversy_ESGD\",\n",
        "    \"EFIV\": \"TotalESG_EFIV\"\n",
        "}\n",
        "\n",
        "price_field = \"Close\"\n",
        "# map price field → PERFORMANCEDATATYPE\n",
        "price2datatype = {\n",
        "    \"Close\":     \"EOD Price\",\n",
        "    \"Adj Close\": \"EOD Price\",\n",
        "    \"Open\":      \"BOD Price\",\n",
        "    \"High\":      \"HIGH Price\",\n",
        "    \"Low\":       \"LOW Price\"\n",
        "}\n",
        "\n",
        "# Optional lookup to turn currency codes into full names\n",
        "currency_full_name = {\n",
        "    \"USD\": \"US Dollar\",\n",
        "    \"EUR\": \"Euro\",\n",
        "    \"GBP\": \"British Pound\",\n",
        "    \"CAD\": \"Canadian Dollar\",\n",
        "    \"JPY\": \"Japanese Yen\"\n",
        "}\n",
        "\n",
        "# ------------------- DATE RANGE -------------------\n",
        "end   = datetime.today()\n",
        "start = datetime(end.year - 10, end.month, end.day)\n",
        "\n",
        "performance_records = []\n",
        "general_info_records = []\n",
        "\n",
        "for symbol, category in benchmark_map.items():\n",
        "    print(f\"Downloading: {symbol}\")\n",
        "    etf = yf.Ticker(symbol)\n",
        "\n",
        "    # Pull 10-year history 📈\n",
        "    hist = etf.history(start=start, end=end)\n",
        "\n",
        "    # Get currency from yfinance; default to USD if missing\n",
        "    cur_code = etf.info.get(\"currency\", \"USD\")\n",
        "    cur_name = currency_full_name.get(cur_code, cur_code)\n",
        "\n",
        "    perf_datatype = price2datatype.get(price_field, \"EOD\")  # fallback to EOD\n",
        "\n",
        "    # --- PERFORMANCE rows ---\n",
        "    for date, row in hist.iterrows():\n",
        "        performance_records.append({\n",
        "            \"BENCHMARKCODE\":            category.upper(),\n",
        "            \"PERFORMANCEDATATYPE\":      perf_datatype,\n",
        "            \"CURRENCYCODE\":             cur_code,\n",
        "            \"CURRENCY\":                 cur_name,\n",
        "            \"PERFORMANCEFREQUENCY\":     \"DAILY\",\n",
        "            \"VALUE\":                    round(row[price_field], 4),\n",
        "            \"HISTORYDATE1\":             date.date(),\n",
        "            \"HISTORYDATE\":              pd.to_datetime(date)\n",
        "        })\n",
        "\n",
        "    # --- GENERAL INFO rows ---\n",
        "    general_info_records.append({\n",
        "        \"BENCHMARKCODE\":            category.upper(),\n",
        "        \"SYMBOL\":                   symbol,\n",
        "        \"NAME\":                     etf.info.get(\"longName\", f\"{symbol} ETF\"),\n",
        "        \"ISBEGINOFDAYPERFORMANCE\":  perf_datatype == \"BOD\"\n",
        "    })\n",
        "\n",
        "# ------------------- DATAFRAMES -------------------\n",
        "df_perf   = pd.DataFrame(performance_records)\n",
        "df_general = pd.DataFrame(general_info_records)\n",
        "\n",
        "print(df_perf.head())\n",
        "print(df_general)\n",
        "\n",
        "# ------------------- SAVE CSVs -------------------\n",
        "df_perf.to_csv(\"benchmark_performance.csv\", index=False)\n",
        "df_general.to_csv(\"benchmark_general_info.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJ6HD2K210j",
        "outputId": "756a780a-6110-4cb4-ee4c-714613f6f432"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: ENRG\n",
            "Downloading: SHE\n",
            "Downloading: VOTE\n",
            "Downloading: ESGD\n",
            "Downloading: EFIV\n",
            "      BENCHMARKCODE PERFORMANCEDATATYPE CURRENCYCODE   CURRENCY  \\\n",
            "0  ENVIRONMENT_ENRG                 EOD          USD  US Dollar   \n",
            "1        SOCIAL_SHE                 EOD          USD  US Dollar   \n",
            "2        SOCIAL_SHE                 EOD          USD  US Dollar   \n",
            "3        SOCIAL_SHE                 EOD          USD  US Dollar   \n",
            "4        SOCIAL_SHE                 EOD          USD  US Dollar   \n",
            "\n",
            "  PERFORMANCEFREQUENCY    VALUE HISTORYDATE1               HISTORYDATE  \n",
            "0                DAILY  25.0000   2025-01-06 2025-01-06 00:00:00-05:00  \n",
            "1                DAILY  45.0149   2016-03-08 2016-03-08 00:00:00-05:00  \n",
            "2                DAILY  44.9921   2016-03-09 2016-03-09 00:00:00-05:00  \n",
            "3                DAILY  44.9807   2016-03-10 2016-03-10 00:00:00-05:00  \n",
            "4                DAILY  45.7225   2016-03-11 2016-03-11 00:00:00-05:00  \n",
            "         BENCHMARKCODE SYMBOL                                NAME  \\\n",
            "0     ENVIRONMENT_ENRG   ENRG                Ninepoint Energy ETF   \n",
            "1           SOCIAL_SHE    SHE  SPDR MSCI USA Gender Diversity ETF   \n",
            "2      GOVERNANCE_VOTE   VOTE               TCW Transform 500 ETF   \n",
            "3  LOWCONTROVERSY_ESGD   ESGD     iShares ESG Aware MSCI EAFE ETF   \n",
            "4        TOTALESG_EFIV   EFIV                SPDR S&P 500 ESG ETF   \n",
            "\n",
            "   ISBEGINOFDAYPERFORMANCE  \n",
            "0                    False  \n",
            "1                    False  \n",
            "2                    False  \n",
            "3                    False  \n",
            "4                    False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to snowflake and load the data directly bypassing any previously loaded data\n",
        "conn = get_snowflake_connection()\n",
        "load_to_snowflake_merge(df_perf, \"SECURITY_MASTER\", conn, unique_keys=[\"symbol\"])"
      ],
      "metadata": {
        "id": "9uPQ5tOUMZhG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZhbkQe_IKVvb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}