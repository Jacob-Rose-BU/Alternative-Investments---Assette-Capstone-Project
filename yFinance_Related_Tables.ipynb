{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacob-Rose-BU/Alternative-Investments---Assette-Capstone-Project/blob/main/yFinance_Related_Tables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ESG Equity Fact Sheet - YFinance Data Pipeline**"
      ],
      "metadata": {
        "id": "HMv5CCePsx5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook pulls real financial data using the Yahoo Finance API via the yfinance library. It generates tables for Security Master, ESG data, and historical price performance for U.S. equities.The extracted data is prepared to be loaded into Snowflake for downstream use in ESG fund fact sheets.\n",
        "\n",
        "\n",
        "###**Execution Instructions**\n",
        "\n",
        "**To run this notebook:**\n",
        "1. Update your Snowflake credentials in the environment or connection file.\n",
        "2. Run the notebook sequentially from top to bottom.\n",
        "\n",
        "### **File Roadmap**\n",
        "Pull S&P 500, NASDAQ 100, Dow Jones tickers <br>\n",
        "Pull yfinance data for valid tickers <br>\n",
        "Extract and clean ESG and performance history <br>\n",
        "Push to Snowflake\n",
        "\n",
        "**Output:** 3 tables in snowflake (security_master, esg_stock_data, stock_performance_history)\n",
        "\n",
        "\n",
        "### **Next Steps**\n",
        "#### **yfinance**\n",
        "- Improve ESG completeness check (what to do when ESG data is not given in yfinance - maybe pull in ESG API)\n",
        "- Add performance benchmark (SPESG & SUSL)\n",
        "\n",
        "####**Snowflake SQl Documentation**\n",
        "- Write code for Holdings creation in Snowflake (Friday Conversation w/ Corey)\n",
        "- Create documentation for reusable steps for top 10 holdings by weight\n",
        "- Create documentation for reusable steps for fund level ESG score aggregation\n",
        "- Create documentation for reusable steps for fund performance versus benchmark\n",
        "\n",
        "### **Future Improvement:**\n",
        "- Automate periodic data refresh\n",
        "- Add additional tickers\n",
        "- Backfull daily performance"
      ],
      "metadata": {
        "id": "KzrnrOqKs4X5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AyWxytZRHOp"
      },
      "source": [
        "# **Connect to Snowflake**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load data into Snowflake, we established a secure connection using credentials stored in a .env file. This connection allows us to push data directly from Python. The pipeline is designed to check if tables already exist, create them if needed, and merge new data while avoiding duplicates. This setup enables seamless integration between our local data processing and Snowflake's cloud warehouse, supporting scalable, centralized storage for downstream analytics like ESG reporting and fact sheet generation."
      ],
      "metadata": {
        "id": "HoH69l85ln6E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "KGL2g66CSJpI",
        "outputId": "18355f9e-c964-48ba-94ad-bd40dd508e18"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9f836f0-1bd0-4767-b58f-65bccc972d9d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9f836f0-1bd0-4767-b58f-65bccc972d9d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env.txt to .env.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.env.txt': b'SNOWFLAKE_ACCOUNT=assette-ssappoc\\nSNOWFLAKE_USER=CRYSTALL\\nSNOWFLAKE_PASSWORD=Bbnmghjtyu123!\\nSNOWFLAKE_ROLE=AST_ALTERNATIVES_DB_RW\\nSNOWFLAKE_WAREHOUSE=AST_BU_WH\\nSNOWFLAKE_DATABASE=AST_ALTERNATIVES_DB\\nSNOWFLAKE_SCHEMA=DBO'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#load the .env file\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi-7qVgwAzeB",
        "outputId": "d96011fb-591a-4066-fa32-2ad9227db1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed .env.txt to .env\n"
          ]
        }
      ],
      "source": [
        "#rename the file if needed\n",
        "import os\n",
        "\n",
        "if os.path.exists(\".env.txt\"):\n",
        "    os.rename(\".env.txt\", \".env\")\n",
        "    print(\"Renamed .env.txt to .env\")\n",
        "else:\n",
        "    print(\"File not found. Make sure you uploaded .env.txt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEenJNbQA1Iz",
        "outputId": "1a69ba60-2c78-4c5c-fb33-8e0a50eadf73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
            "  Downloading boto3-1.40.3-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
            "  Downloading botocore-1.40.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.14.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.8)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (0.13.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
            "Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.3-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, python-dotenv, jmespath, botocore, s3transfer, boto3, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 boto3-1.40.3 botocore-1.40.3 jmespath-1.0.1 python-dotenv-1.1.1 s3transfer-0.13.1 snowflake-connector-python-3.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install snowflake-connector-python python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUlGi_CSA2yw",
        "outputId": "33f09cdb-5358-41df-9a37-7b7aa488fe76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import snowflake.connector\n",
        "\n",
        "# Load .env file data\n",
        "load_dotenv(\".env\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2AuY5_nkA512"
      },
      "outputs": [],
      "source": [
        "#use .env paramaters to connect to snowflake\n",
        "def get_snowflake_connection():\n",
        "    return snowflake.connector.connect(\n",
        "        user=os.getenv(\"SNOWFLAKE_USER\"),\n",
        "        password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "        account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "        role=os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "        warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "        database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "        schema=os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
        "    )\n",
        "#connection - connection is authenticated\n",
        "connection = get_snowflake_connection()\n",
        "#lets me run SQL commands\n",
        "cursor = connection.cursor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PyoGZIlcDlwY"
      },
      "outputs": [],
      "source": [
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "\n",
        "def safe_quote(col: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensures column names are safely quoted for Snowflake SQL syntax.\n",
        "    Replaces internal quotes and wraps the name in double quotes.\n",
        "    \"\"\"\n",
        "    col = str(col).replace('\"', '\"\"').strip()\n",
        "    return f'\"{col}\"'\n",
        "\n",
        "def map_dtype_to_snowflake(dtype):\n",
        "    \"\"\"\n",
        "    Maps pandas dtypes to Snowflake SQL data types.\n",
        "    \"\"\"\n",
        "    if pd.api.types.is_float_dtype(dtype):\n",
        "        return \"FLOAT\"\n",
        "    elif pd.api.types.is_integer_dtype(dtype):\n",
        "        return \"NUMBER\"\n",
        "    else:\n",
        "        return \"VARCHAR\"\n",
        "\n",
        "def load_to_snowflake_merge(df, table_name, conn, unique_keys):\n",
        "    \"\"\"\n",
        "    Uploads DataFrame to Snowflake with type inference and merge logic.\n",
        "    \"\"\"\n",
        "    cur = conn.cursor()\n",
        "    df_cols = df.columns.tolist()\n",
        "    temp_table = f\"{table_name}_STAGING\"\n",
        "\n",
        "    # Step 1: Infer column types and create table if needed\n",
        "    col_defs = \", \".join([\n",
        "        f\"{safe_quote(col)} {map_dtype_to_snowflake(df[col].dtype)}\"\n",
        "        for col in df_cols\n",
        "    ])\n",
        "    cur.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} ({col_defs})\")\n",
        "\n",
        "    # Step 2: Add any missing columns to the main table\n",
        "    cur.execute(f\"DESC TABLE {table_name}\")\n",
        "    existing_cols = {row[0].upper() for row in cur.fetchall()}\n",
        "    for col in df_cols:\n",
        "        if col.upper() not in existing_cols:\n",
        "            col_type = map_dtype_to_snowflake(df[col].dtype)\n",
        "            cur.execute(f\"ALTER TABLE {table_name} ADD COLUMN {safe_quote(col)} {col_type}\")\n",
        "\n",
        "    # Step 3: Create staging table\n",
        "    cur.execute(f\"CREATE OR REPLACE TABLE {temp_table} ({col_defs})\")\n",
        "    write_pandas(conn, df, temp_table)\n",
        "\n",
        "    # Step 4: Merge without duplication\n",
        "    on_clause = \" AND \".join([f\"t.{safe_quote(col)} = s.{safe_quote(col)}\" for col in unique_keys])\n",
        "    insert_cols = \", \".join([safe_quote(col) for col in df_cols])\n",
        "    insert_vals = \", \".join([f\"s.{safe_quote(col)}\" for col in df_cols])\n",
        "\n",
        "    merge_stmt = f\"\"\"\n",
        "        MERGE INTO {table_name} t\n",
        "        USING {temp_table} s\n",
        "        ON {on_clause}\n",
        "        WHEN NOT MATCHED THEN\n",
        "            INSERT ({insert_cols}) VALUES ({insert_vals})\n",
        "    \"\"\"\n",
        "    cur.execute(merge_stmt)\n",
        "\n",
        "    # Step 5: Clean up\n",
        "    cur.execute(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"{table_name} updated. Duplicates prevented using keys: {unique_keys}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktPolCY7A8Nq",
        "outputId": "2b69d866-b997-4aae-ee83-7a0cd54e4501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('CRYSTALL', 'AST_ALTERNATIVES_DB_RW', 'AST_ALTERNATIVES_DB', datetime.date(2025, 8, 5))\n"
          ]
        }
      ],
      "source": [
        "#test the connection\n",
        "cursor.execute(\"SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_DATE;\")\n",
        "\n",
        "for row in cursor:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1RCeSYqBDAZQ"
      },
      "outputs": [],
      "source": [
        "#close SQL cursor\n",
        "cursor.close()\n",
        "#close connection to snowflake\n",
        "connection.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Securities List**"
      ],
      "metadata": {
        "id": "ZhbkQe_IKVvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a clean and verified list of stocks by scraping 3 major US equity indices from Wikipedia: the S&P500, Dow Jones Industrial Average, and NASDAQ 100. Each of these index lists are retried through BeautifulSoup. The extracted tickers are combined into a single list, cleaned, conform to the expected format, and deduplicated between the 3 indexes. To ensure only valid tickers are included a function was defined to check that Yahoo Finance returns metadata.\n",
        "<br> <br>\n",
        "The tickers chosen (S&P500, Dow Jones Industrial Average, NASDAQ 100) represent large, liquid, and well known US companies. They are likely to be included in popular retail and institutional funds, making them a reasonable starting point for building fund simulations and a security master. The decision to limit the scope to these indives was intentional, by focusing on high confdence symbols, the code minimizes errors and avoids excessive querying that could trigger rate limits or bands fron the Yahoo Finance API."
      ],
      "metadata": {
        "id": "5TyqHAdOZ1uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "#get tickers from sp500\n",
        "def get_sp500_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
        "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
        "    return [row.find_all(\"td\")[0].text.strip() for row in table.find_all(\"tr\")[1:]]\n",
        "\n",
        "#get tickers from dow jones indstrial\n",
        "def get_dow_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average\"\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
        "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
        "    return [\n",
        "        row.find_all(\"td\")[1].find(\"a\").text.strip()\n",
        "        for row in table.find_all(\"tr\")[1:]\n",
        "        if len(row.find_all(\"td\")) >= 2\n",
        "    ]\n",
        "\n",
        "#get tickers from NASDAQ-100\n",
        "def get_nasdaq100_tickers():\n",
        "    url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n",
        "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
        "    table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
        "    return [\n",
        "        row.find_all(\"td\")[0].text.strip()\n",
        "        for row in table.find_all(\"tr\")[1:]\n",
        "        if len(row.find_all(\"td\")) >= 1\n",
        "    ]\n",
        "\n",
        "\n",
        "#validate that these are real tickers\n",
        "def is_valid_ticker(ticker):\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return \"shortName\" in info\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "#pull all tickers from all of the above sources\n",
        "sp500 = get_sp500_tickers()\n",
        "dow = get_dow_tickers()\n",
        "nasdaq = get_nasdaq100_tickers()\n",
        "\n",
        "#combine and make tickers unique\n",
        "all_tickers = sorted(set(sp500 + dow + nasdaq))\n",
        "\n",
        "#data cleaning\n",
        "all_tickers = [t.replace('.', '-') for t in all_tickers]\n"
      ],
      "metadata": {
        "id": "Op7Ze24HKVJJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Insert yFinance Data into Snowflake Tables**\n"
      ],
      "metadata": {
        "id": "ayhdSdLxr5Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed to build 3 key financial datasets, a security master table, ESG scores, and 10 years of historical price performance, for a list of securities. Using the Yahoo Finance API, the script loops through each ticker and retrieves metadata like company name, sector,, industry, market cap, and trading exchanges. It also attemps to fetch ESG related metrics (if available) and daily price history over the past 10 years. All this information is stored in separate DataFrames. If a ticker fails to return valid metadata or historical price data, it is logged into a failed ticker list. Once the data is collected, the script standardizes data formatting and pushes each DataFrame directly to Snowflake using a merg strategy, avoiding duplicates based on defined unique keys. <br> <br>\n",
        "Initally, the code included additional tables such as price snapshots, fundamentals, and analyst estimates. But, we decided to remove these from the pipeline because they aren't directly used in our target deliverable, the fund fact sheet. While they may be used in the broader portfolio analytics or internal risk assessments, they were out of scope for this specific task. <br>\n",
        "The security master table is foundational to the pipeline, as it centralizes all core attributes about the securities in our dataset. It ensures consistency and enables future joins with holdings, ESG metrics, and price data. We also collect daily price history over a 10 year horizon to support fund level performance analysis, invluding quarter over quarter or year over year changes. Ideally, this time range should be extended further to reflect real world investment horizons more accurately. However, we limited the query to 10 years to avoid triggering API rate limits of blocks from Yahoo Finance. A long-term enhancement would be periodically refresh historical data and biild a more robust price history system over time. <br> <br>\n",
        "While this current version is designed to run once and populate Snowflake, future iterations could introduce automated checks. Fo rexample, if a security appears in holdings but is missing from the security master, the system should automatically fetch and populate its metadata from Yahoo Finance. This would ensure the pipeline remains dynamic and scalable as fund compositions evolve."
      ],
      "metadata": {
        "id": "CRmKH6HuYJgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#14 mins to run\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Initialize lists\n",
        "security_master = []\n",
        "combined_data = []\n",
        "failed_tickers = []\n",
        "\n",
        "# --- Date range for historical price data (last 10 years) ---\n",
        "start_date = (datetime.datetime.today() - datetime.timedelta(days=365 * 10)).strftime('%Y-%m-%d')\n",
        "end_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "for symbol in all_tickers:\n",
        "    try:\n",
        "        t = yf.Ticker(symbol)\n",
        "        info = t.info\n",
        "\n",
        "        # Skip invalid tickers\n",
        "        if not info or \"shortName\" not in info:\n",
        "            print(f\"No valid info for {symbol}\")\n",
        "            failed_tickers.append(symbol)\n",
        "            continue\n",
        "\n",
        "        # --- Security Master ---\n",
        "        security_master.append({\n",
        "            \"ticker\": symbol,\n",
        "            \"shortName\": info.get(\"shortName\"),\n",
        "            \"name\": info.get(\"longName\"),\n",
        "            \"sector\": info.get(\"sector\"),\n",
        "            \"industry\": info.get(\"industry\"),\n",
        "            \"exchange\": info.get(\"exchange\"),\n",
        "            \"currency\": info.get(\"currency\"),\n",
        "            \"country\": info.get(\"country\"),\n",
        "            \"market_cap\": info.get(\"marketCap\")\n",
        "        })\n",
        "\n",
        "        # --- Historical Performance ---\n",
        "        hist = t.history(start=start_date, end=end_date)\n",
        "        if hist.empty:\n",
        "            print(f\"No price history for {symbol}\")\n",
        "            failed_tickers.append(symbol)\n",
        "            continue\n",
        "\n",
        "        hist = hist.reset_index()\n",
        "        hist[\"ticker\"] = symbol\n",
        "        hist[\"data_type\"] = \"price\"\n",
        "        combined_data.append(hist)\n",
        "\n",
        "        # --- ESG as single-row record ---\n",
        "        sustainability = t.sustainability\n",
        "        if sustainability is not None and not sustainability.empty:\n",
        "            row = sustainability.transpose()\n",
        "            esg_row = {\n",
        "                \"Date\": pd.to_datetime(\"today\"),\n",
        "                \"Open\": None,\n",
        "                \"High\": None,\n",
        "                \"Low\": None,\n",
        "                \"Close\": None,\n",
        "                \"Volume\": None,\n",
        "                \"Dividends\": None,\n",
        "                \"Stock Splits\": None,\n",
        "                \"ticker\": symbol,\n",
        "                \"data_type\": \"esg\",\n",
        "                \"esgPerformance\": row.get(\"esgPerformance\", {}).values[0] if \"esgPerformance\" in row else None,\n",
        "                \"totalEsg\": row.get(\"totalEsg\", {}).values[0] if \"totalEsg\" in row else None,\n",
        "                \"environmentScore\": row.get(\"environmentScore\", {}).values[0] if \"environmentScore\" in row else None,\n",
        "                \"socialScore\": row.get(\"socialScore\", {}).values[0] if \"socialScore\" in row else None,\n",
        "                \"governanceScore\": row.get(\"governanceScore\", {}).values[0] if \"governanceScore\" in row else None,\n",
        "                \"highestControversy\": row.get(\"highestControversy\", {}).values[0] if \"highestControversy\" in row else None\n",
        "            }\n",
        "            combined_data.append(pd.DataFrame([esg_row]))\n",
        "\n",
        "        # Sleep to avoid hitting API rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {symbol}: {e}\")\n",
        "        failed_tickers.append(symbol)\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_security_master = pd.DataFrame(security_master)\n",
        "df_performance = pd.concat(combined_data, ignore_index=True) if combined_data else pd.DataFrame()\n",
        "\n",
        "df_performance[\"Date\"] = pd.to_datetime(df_performance[\"Date\"], utc=True).dt.tz_localize(None).dt.date\n",
        "\n",
        "\n",
        "# --- Output summary ---\n",
        "print(f\"\\n Finished processing {len(all_tickers)} tickers.\")\n",
        "print(f\" Failed tickers: {len(failed_tickers)}\")\n",
        "print(failed_tickers)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQz1awf9Qvsi",
        "outputId": "dec63d21-9ed0-475c-8b63-e1752b79776c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "/tmp/ipython-input-1906262051.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_performance = pd.concat(combined_data, ignore_index=True) if combined_data else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Finished processing 517 tickers.\n",
            " Failed tickers: 0\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to snowflake and load the data directly bypassing any previously loaded data\n",
        "conn = get_snowflake_connection()\n",
        "load_to_snowflake_merge(df_security_master, \"SECURITY_MASTER\", conn, unique_keys=[\"ticker\"])\n",
        "\n",
        "conn = get_snowflake_connection()\n",
        "if not df_performance.empty:\n",
        "    load_to_snowflake_merge(df_performance, \"SECURITY_PERFORMANCE_HISTORY\", conn, unique_keys=[\"Date\", \"ticker\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxYngE3btHjM",
        "outputId": "21237d95-06ab-44df-f4cd-393de1857fb4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SECURITY_MASTER updated. Duplicates prevented using keys: ['ticker']\n",
            "SECURITY_PERFORMANCE_HISTORY updated. Duplicates prevented using keys: ['Date', 'ticker']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **yFinance Benchmark Indexes**"
      ],
      "metadata": {
        "id": "xtfCqfKZ2yOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "theres no esg score for indexes\n",
        "chose one index for each fund focus.\n",
        "initally hardcoded some column fields but in order to make it repeatable and applied to other areas then did a config. also maybe reference currency info from the currency table instead of currency_full_name.\n",
        "\n",
        "add this information into the 2-3 tables that are available for benchmarks.\n",
        "\n",
        "make its own table in snowflake. do a join between benchmark tables and"
      ],
      "metadata": {
        "id": "NJtQc4DDTsCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Define benchmark tickers\n",
        "benchmark_tickers = [\"ENRG\", \"SHE\", \"VOTE\", \"ESGD\", \"EFIV\"]\n",
        "\n",
        "# Define date range (last 3 years)\n",
        "start_date = datetime.datetime.today() - datetime.timedelta(days=365 * 10)\n",
        "end_date = datetime.datetime.today()\n",
        "\n",
        "# Initialize containers\n",
        "benchmark_performance = []\n",
        "benchmark_general_information = []\n",
        "benchmark_characteristics = []\n",
        "\n",
        "# Pull data for each benchmark\n",
        "for ticker in benchmark_tickers:\n",
        "    try:\n",
        "        t = yf.Ticker(ticker)\n",
        "        info = t.info\n",
        "        hist = t.history(start=start_date, end=end_date)\n",
        "\n",
        "        if hist.empty or not info:\n",
        "            continue\n",
        "\n",
        "        # === Benchmark Performance ===\n",
        "        hist = hist.reset_index()\n",
        "        df_perf = pd.DataFrame({\n",
        "            \"BENCHMARKCODE\": ticker.lower(),\n",
        "            \"PERFORMANCETYPE\": \"Prices\",\n",
        "            \"CURRENCYCODE\": info.get(\"currency\", \"USD\"),\n",
        "            \"CURRENCY\": info.get(\"financialCurrency\", info.get(\"currency\", \"USD\")),\n",
        "            \"PERFORMANCEFREQUENCY\": \"Daily\",\n",
        "            \"VALUE\": hist[\"Close\"],\n",
        "            \"HISTORYDATE1\": hist[\"Date\"].dt.date,\n",
        "            \"HISTORYDATE\": hist[\"Date\"]\n",
        "        })\n",
        "        benchmark_performance.append(df_perf)\n",
        "\n",
        "        # === Benchmark General Information ===\n",
        "        benchmark_general_information.append({\n",
        "            \"BENCHMARKCODE\": ticker.lower(),\n",
        "            \"TICKER\": ticker,\n",
        "            \"NAME\": info.get(\"longName\", info.get(\"shortName\", ticker)),\n",
        "            \"ISBEGINOFDAYPERFORMANCE\": False\n",
        "        })\n",
        "\n",
        "        # === Benchmark Characteristics (auto-detect numeric fields) ===\n",
        "        for key, value in info.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                benchmark_characteristics.append({\n",
        "                    \"BENCHMARKCODE\": ticker.lower(),\n",
        "                    \"CURRENCYCODE\": info.get(\"currency\", \"USD\"),\n",
        "                    \"CURRENCY\": info.get(\"financialCurrency\", info.get(\"currency\", \"USD\")),\n",
        "                    \"LANGUAGECODE\": \"en-US\",\n",
        "                    \"CATEGORY\": \"Total\",\n",
        "                    \"CATEGORYNAME\": None,\n",
        "                    \"CHARACTERISTICNAME\": key,\n",
        "                    \"CHARACTERISTICDISPLAYNAME\": key.replace('_', ' ').title(),\n",
        "                    \"STATISTICTYPE\": \"NA\",\n",
        "                    \"CHARACTERISTICVALUE\": value,\n",
        "                    \"ABBREVIATEDTEXT\": None,\n",
        "                    \"HISTORYDATE\": datetime.date.today()\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error with {ticker}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HDHFcRLwaczA"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrames\n",
        "df_benchmark_performance = pd.concat(benchmark_performance, ignore_index=True)\n",
        "df_benchmark_general_info = pd.DataFrame(benchmark_general_information)\n",
        "df_benchmark_characteristics = pd.DataFrame(benchmark_characteristics)\n",
        "\n",
        "# Preview\n",
        "print(\" Performance sample:\")\n",
        "print(df_benchmark_performance.head(3))\n",
        "print(\"\\n General Info sample:\")\n",
        "print(df_benchmark_general_info.head(3))\n",
        "print(\"\\n Characteristics sample:\")\n",
        "print(df_benchmark_characteristics.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2wYGe87V_sA",
        "outputId": "9d520c37-e0e2-4a34-b4e7-fd871f9ff136"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Performance sample:\n",
            "  BENCHMARKCODE PERFORMANCETYPE CURRENCYCODE CURRENCY PERFORMANCEFREQUENCY  \\\n",
            "0          enrg          Prices          USD      USD                Daily   \n",
            "1           she          Prices          USD      USD                Daily   \n",
            "2           she          Prices          USD      USD                Daily   \n",
            "\n",
            "       VALUE HISTORYDATE1               HISTORYDATE  \n",
            "0  25.000000   2025-01-06 2025-01-06 00:00:00-05:00  \n",
            "1  45.014851   2016-03-08 2016-03-08 00:00:00-05:00  \n",
            "2  44.992115   2016-03-09 2016-03-09 00:00:00-05:00  \n",
            "\n",
            " General Info sample:\n",
            "  BENCHMARKCODE TICKER                                NAME  \\\n",
            "0          enrg   ENRG                Ninepoint Energy ETF   \n",
            "1           she    SHE  SPDR MSCI USA Gender Diversity ETF   \n",
            "2          vote   VOTE               TCW Transform 500 ETF   \n",
            "\n",
            "   ISBEGINOFDAYPERFORMANCE  \n",
            "0                    False  \n",
            "1                    False  \n",
            "2                    False  \n",
            "\n",
            " Characteristics sample:\n",
            "  BENCHMARKCODE CURRENCYCODE CURRENCY LANGUAGECODE CATEGORY CATEGORYNAME  \\\n",
            "0          enrg          USD      USD        en-US    Total         None   \n",
            "1          enrg          USD      USD        en-US    Total         None   \n",
            "2          enrg          USD      USD        en-US    Total         None   \n",
            "\n",
            "  CHARACTERISTICNAME CHARACTERISTICDISPLAYNAME STATISTICTYPE  \\\n",
            "0             maxAge                    Maxage            NA   \n",
            "1          priceHint                 Pricehint            NA   \n",
            "2      previousClose             Previousclose            NA   \n",
            "\n",
            "  CHARACTERISTICVALUE ABBREVIATEDTEXT HISTORYDATE  \n",
            "0               86400            None  2025-08-06  \n",
            "1                   2            None  2025-08-06  \n",
            "2             12.6065            None  2025-08-06  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HoldingsDetails & PortfolioPerformance**\n"
      ],
      "metadata": {
        "id": "wNk28qJEUSPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "holdingsdetails does not have all, just has the most important fields. also added some fields to portfolioperformance as well to reflect ESG scores and the portfoliofocus"
      ],
      "metadata": {
        "id": "-vB9FzQLZY-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Normalize 'Date' column\n",
        "df_performance.rename(columns=lambda x: x.strip().capitalize() if x.lower() == 'date' else x, inplace=True)\n",
        "\n",
        "# Step 1: Filter valid symbols\n",
        "valid_symbols = df_security_master['ticker'].unique()\n",
        "\n",
        "# Step 2: Extract ESG and price data\n",
        "df_esg = df_performance[df_performance['data_type'] == 'esg'].copy()\n",
        "df_esg = df_esg[df_esg['ticker'].isin(valid_symbols)]\n",
        "df_esg_latest = df_esg.sort_values('Date').drop_duplicates('ticker', keep='last')\n",
        "\n",
        "df_price = df_performance[df_performance['data_type'] == 'price'].copy()\n",
        "df_price = df_price[df_price['ticker'].isin(valid_symbols)]\n",
        "df_price_latest = df_price.sort_values('Date').drop_duplicates('ticker', keep='last')\n",
        "df_price_latest = df_price_latest[['ticker', 'Date', 'Close']].rename(columns={'Close': 'price'})\n",
        "\n",
        "# Step 3: Define synthetic funds\n",
        "funds = pd.DataFrame({\n",
        "    \"PORTFOLIOCODE\": [\n",
        "        \"Climate_Leaders_Fund\", \"Social_Impact_Fund\", \"Governance_Focused_Fund\",\n",
        "        \"Low_Controversy_Fund\", \"Overall_ESG_Leaders\", \"Stakeholder_Advocacy_Fund\"\n",
        "    ],\n",
        "    \"PORTFOLIOFOCUS\": [\n",
        "        \"environmentScore\", \"socialScore\", \"governanceScore\",\n",
        "        \"highestControversy\", \"totalEsg\", \"socialScore\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Step 4: Generate holdingsdetails table\n",
        "fund_value_usd = 100_000_000\n",
        "holdings_rows = []\n",
        "\n",
        "for _, fund in funds.iterrows():\n",
        "    focus = fund[\"PORTFOLIOFOCUS\"]\n",
        "    df = df_esg_latest.copy()\n",
        "\n",
        "    # Correct sorting logic: lower score = better, except highestControversy\n",
        "    df = df.sort_values(focus, ascending=(focus != 'highestControversy')).dropna(subset=[focus]).head(20).copy()\n",
        "\n",
        "\n",
        "    df[\"raw_weight\"] = np.abs(np.random.rand(len(df)))\n",
        "    df = df.merge(df_price_latest, on='ticker', how='left').dropna(subset=[\"price\"])\n",
        "    df[\"weight\"] = df[\"raw_weight\"] / df[\"raw_weight\"].sum()\n",
        "\n",
        "\n",
        "    df_meta = df.merge(df_security_master, on='ticker', how='left')\n",
        "\n",
        "    for _, row in df_meta.iterrows():\n",
        "        invested = fund_value_usd * row[\"weight\"]\n",
        "        shares = invested / row[\"price\"]\n",
        "        holdings_rows.append({\n",
        "            \"PORTFOLIOCODE\": fund[\"PORTFOLIOCODE\"],\n",
        "            \"CURRENCYCODE\": row[\"currency\"],\n",
        "            \"CURRENCY\": \"US Dollar\",\n",
        "            \"ISSUENAME\": row[\"name\"],\n",
        "            \"TICKER\": row[\"ticker\"],\n",
        "            \"QUANTITY\": round(shares, 2),\n",
        "            \"MARKETVALUE\": round(invested, 2),\n",
        "            \"PORTFOLIOWEIGHT\": round(row[\"weight\"] , 6),\n",
        "            \"PRICE\": round(row[\"price\"], 2),\n",
        "            \"ASSETCLASSNAME\": row[\"sector\"],\n",
        "            \"ISSUETYPE\": row[\"industry\"],\n",
        "            \"ISSUECOUNTRYCODE\": row[\"exchange\"],\n",
        "            \"ISSUECOUNTRY\": row[\"country\"],\n",
        "            \"HISTORYDATE\": row[\"Date_x\"]\n",
        "        })\n",
        "\n",
        "df_holdingsdetails = pd.DataFrame(holdings_rows)\n",
        "\n",
        "# Step 5: Create portfolioperformance table\n",
        "performance_date = pd.to_datetime(\"2025-06-30\")\n",
        "inception_date = pd.to_datetime(\"2023-01-01\")\n",
        "\n",
        "# Compute ESG averages based on fund_focus\n",
        "avg_scores = []\n",
        "for _, fund in funds.iterrows():\n",
        "    focus = fund[\"PORTFOLIOFOCUS\"]\n",
        "    symbols = df_holdingsdetails[df_holdingsdetails[\"PORTFOLIOCODE\"] == fund[\"PORTFOLIOCODE\"]][\"TICKER\"]\n",
        "    values = df_esg_latest[df_esg_latest[\"ticker\"].isin(symbols)][focus]\n",
        "\n",
        "    if focus == \"highestControversy\":\n",
        "        avg_score = round(values.max(), 2)  # higher is better\n",
        "    else:\n",
        "        avg_score = round(values.min(), 2)  # lower is better\n",
        "\n",
        "    avg_scores.append(avg_score)\n",
        "\n",
        "funds[\"AVERAGE_ESG_SCORE\"] = avg_scores\n",
        "\n",
        "df_portfolioperformance = funds.assign(\n",
        "    HISTORYDATE=performance_date,\n",
        "    CURRENCYCODE=\"USD\",\n",
        "    CURRENCY=\"US Dollar\",\n",
        "    PERFORMANCECATEGORY=\"Asset Class\",\n",
        "    PERFORMANCECATEGORYNAME=\"Total Portfolio\",\n",
        "    PERFORMANCETYPE=\"Portfolio Gross\",\n",
        "    PERFORMANCEINCEPTIONDATE=inception_date,\n",
        "    PORTFOLIOINCEPTIONDATE=inception_date,\n",
        "    PERFORMANCEFREQUENCY=\"D\",\n",
        "    PERFORMANCEFACTOR=np.round(np.random.normal(loc=0.001, scale=0.01, size=len(funds)), 6)\n",
        ")\n",
        "\n",
        "# --- OUTPUT ---\n",
        "print(\" HoldingsDetails:\")\n",
        "print(df_holdingsdetails.head())\n",
        "\n",
        "print(\"\\n PortfolioPerformance:\")\n",
        "print(df_portfolioperformance.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxcr2eEZ_0CV",
        "outputId": "53c61b48-53e5-4d6f-d9f2-dff294eb903e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " HoldingsDetails:\n",
            "          PORTFOLIOCODE CURRENCYCODE   CURRENCY  \\\n",
            "0  Climate_Leaders_Fund          USD  US Dollar   \n",
            "1  Climate_Leaders_Fund          USD  US Dollar   \n",
            "2  Climate_Leaders_Fund          USD  US Dollar   \n",
            "3  Climate_Leaders_Fund          USD  US Dollar   \n",
            "4  Climate_Leaders_Fund          USD  US Dollar   \n",
            "\n",
            "                                  ISSUENAME TICKER  QUANTITY  MARKETVALUE  \\\n",
            "0                                   Aon plc    AON  25488.99   9216817.76   \n",
            "1  The Interpublic Group of Companies, Inc.    IPG  92346.06   2284641.60   \n",
            "2                  Palo Alto Networks, Inc.   PANW  39655.22   6705301.53   \n",
            "3                               DaVita Inc.    DVA   1389.02    195184.52   \n",
            "4                              Nasdaq, Inc.   NDAQ  63989.93   6164789.67   \n",
            "\n",
            "   PORTFOLIOWEIGHT   PRICE          ASSETCLASSNAME  \\\n",
            "0         0.092168  361.60      Financial Services   \n",
            "1         0.022846   24.74  Communication Services   \n",
            "2         0.067053  169.09              Technology   \n",
            "3         0.001952  140.52              Healthcare   \n",
            "4         0.061648   96.34      Financial Services   \n",
            "\n",
            "                          ISSUETYPE ISSUECOUNTRYCODE   ISSUECOUNTRY  \\\n",
            "0                 Insurance Brokers              NYQ        Ireland   \n",
            "1              Advertising Agencies              NYQ  United States   \n",
            "2         Software - Infrastructure              NMS  United States   \n",
            "3           Medical Care Facilities              NYQ  United States   \n",
            "4  Financial Data & Stock Exchanges              NMS  United States   \n",
            "\n",
            "  HISTORYDATE  \n",
            "0  2025-08-06  \n",
            "1  2025-08-06  \n",
            "2  2025-08-06  \n",
            "3  2025-08-06  \n",
            "4  2025-08-06  \n",
            "\n",
            " PortfolioPerformance:\n",
            "             PORTFOLIOCODE      PORTFOLIOFOCUS  AVERAGE_ESG_SCORE HISTORYDATE  \\\n",
            "0     Climate_Leaders_Fund    environmentScore               0.04  2025-06-30   \n",
            "1       Social_Impact_Fund         socialScore               0.89  2025-06-30   \n",
            "2  Governance_Focused_Fund     governanceScore               1.68  2025-06-30   \n",
            "3     Low_Controversy_Fund  highestControversy               5.00  2025-06-30   \n",
            "4      Overall_ESG_Leaders            totalEsg               7.28  2025-06-30   \n",
            "\n",
            "  CURRENCYCODE   CURRENCY PERFORMANCECATEGORY PERFORMANCECATEGORYNAME  \\\n",
            "0          USD  US Dollar         Asset Class         Total Portfolio   \n",
            "1          USD  US Dollar         Asset Class         Total Portfolio   \n",
            "2          USD  US Dollar         Asset Class         Total Portfolio   \n",
            "3          USD  US Dollar         Asset Class         Total Portfolio   \n",
            "4          USD  US Dollar         Asset Class         Total Portfolio   \n",
            "\n",
            "   PERFORMANCETYPE PERFORMANCEINCEPTIONDATE PORTFOLIOINCEPTIONDATE  \\\n",
            "0  Portfolio Gross               2023-01-01             2023-01-01   \n",
            "1  Portfolio Gross               2023-01-01             2023-01-01   \n",
            "2  Portfolio Gross               2023-01-01             2023-01-01   \n",
            "3  Portfolio Gross               2023-01-01             2023-01-01   \n",
            "4  Portfolio Gross               2023-01-01             2023-01-01   \n",
            "\n",
            "  PERFORMANCEFREQUENCY  PERFORMANCEFACTOR  \n",
            "0                    D           0.011725  \n",
            "1                    D          -0.001496  \n",
            "2                    D          -0.009775  \n",
            "3                    D          -0.007107  \n",
            "4                    D          -0.003605  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataFrame Summary**\n"
      ],
      "metadata": {
        "id": "WA48kUajlX3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\" df_security_master:\")\n",
        "print(df_security_master.info())\n",
        "print(\" df_performance:\")\n",
        "print(df_performance.info())\n",
        "print(\" df_benchmark_performance:\")\n",
        "print(df_benchmark_performance.info())\n",
        "print(\" df_benchmark_general_info:\")\n",
        "print(df_benchmark_general_info.info())\n",
        "print(\" df_benchmark_characteristics:\")\n",
        "print(df_benchmark_characteristics.info())\n",
        "print(\" df_holdingsdetails:\")\n",
        "print(df_holdingsdetails.info())\n",
        "print(\" df_portfolioperformance:\")\n",
        "print(df_portfolioperformance.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8-SKaFdlcUr",
        "outputId": "326e92a4-662c-4edf-c9b1-96c1bc4742bc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " df_security_master:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 517 entries, 0 to 516\n",
            "Data columns (total 9 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   ticker      517 non-null    object\n",
            " 1   shortName   517 non-null    object\n",
            " 2   name        517 non-null    object\n",
            " 3   sector      517 non-null    object\n",
            " 4   industry    517 non-null    object\n",
            " 5   exchange    517 non-null    object\n",
            " 6   currency    517 non-null    object\n",
            " 7   country     517 non-null    object\n",
            " 8   market_cap  517 non-null    int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 36.5+ KB\n",
            "None\n",
            " df_performance:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1260395 entries, 0 to 1260394\n",
            "Data columns (total 16 columns):\n",
            " #   Column              Non-Null Count    Dtype  \n",
            "---  ------              --------------    -----  \n",
            " 0   Date                1260395 non-null  object \n",
            " 1   Open                1259884 non-null  float64\n",
            " 2   High                1259884 non-null  float64\n",
            " 3   Low                 1259884 non-null  float64\n",
            " 4   Close               1259884 non-null  float64\n",
            " 5   Volume              1259884 non-null  object \n",
            " 6   Dividends           1259884 non-null  float64\n",
            " 7   Stock Splits        1259884 non-null  float64\n",
            " 8   ticker              1260395 non-null  object \n",
            " 9   data_type           1260395 non-null  object \n",
            " 10  esgPerformance      511 non-null      object \n",
            " 11  totalEsg            511 non-null      float64\n",
            " 12  environmentScore    509 non-null      float64\n",
            " 13  socialScore         509 non-null      float64\n",
            " 14  governanceScore     509 non-null      float64\n",
            " 15  highestControversy  511 non-null      float64\n",
            "dtypes: float64(11), object(5)\n",
            "memory usage: 153.9+ MB\n",
            "None\n",
            " df_benchmark_performance:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6911 entries, 0 to 6910\n",
            "Data columns (total 8 columns):\n",
            " #   Column                Non-Null Count  Dtype                           \n",
            "---  ------                --------------  -----                           \n",
            " 0   BENCHMARKCODE         6911 non-null   object                          \n",
            " 1   PERFORMANCETYPE       6911 non-null   object                          \n",
            " 2   CURRENCYCODE          6911 non-null   object                          \n",
            " 3   CURRENCY              6911 non-null   object                          \n",
            " 4   PERFORMANCEFREQUENCY  6911 non-null   object                          \n",
            " 5   VALUE                 6911 non-null   float64                         \n",
            " 6   HISTORYDATE1          6911 non-null   object                          \n",
            " 7   HISTORYDATE           6911 non-null   datetime64[ns, America/New_York]\n",
            "dtypes: datetime64[ns, America/New_York](1), float64(1), object(6)\n",
            "memory usage: 432.1+ KB\n",
            "None\n",
            " df_benchmark_general_info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column                   Non-Null Count  Dtype \n",
            "---  ------                   --------------  ----- \n",
            " 0   BENCHMARKCODE            5 non-null      object\n",
            " 1   TICKER                   5 non-null      object\n",
            " 2   NAME                     5 non-null      object\n",
            " 3   ISBEGINOFDAYPERFORMANCE  5 non-null      bool  \n",
            "dtypes: bool(1), object(3)\n",
            "memory usage: 257.0+ bytes\n",
            "None\n",
            " df_benchmark_characteristics:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 315 entries, 0 to 314\n",
            "Data columns (total 12 columns):\n",
            " #   Column                     Non-Null Count  Dtype \n",
            "---  ------                     --------------  ----- \n",
            " 0   BENCHMARKCODE              315 non-null    object\n",
            " 1   CURRENCYCODE               315 non-null    object\n",
            " 2   CURRENCY                   315 non-null    object\n",
            " 3   LANGUAGECODE               315 non-null    object\n",
            " 4   CATEGORY                   315 non-null    object\n",
            " 5   CATEGORYNAME               0 non-null      object\n",
            " 6   CHARACTERISTICNAME         315 non-null    object\n",
            " 7   CHARACTERISTICDISPLAYNAME  315 non-null    object\n",
            " 8   STATISTICTYPE              315 non-null    object\n",
            " 9   CHARACTERISTICVALUE        315 non-null    object\n",
            " 10  ABBREVIATEDTEXT            0 non-null      object\n",
            " 11  HISTORYDATE                315 non-null    object\n",
            "dtypes: object(12)\n",
            "memory usage: 29.7+ KB\n",
            "None\n",
            " df_holdingsdetails:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 120 entries, 0 to 119\n",
            "Data columns (total 14 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   PORTFOLIOCODE     120 non-null    object \n",
            " 1   CURRENCYCODE      120 non-null    object \n",
            " 2   CURRENCY          120 non-null    object \n",
            " 3   ISSUENAME         120 non-null    object \n",
            " 4   TICKER            120 non-null    object \n",
            " 5   QUANTITY          120 non-null    float64\n",
            " 6   MARKETVALUE       120 non-null    float64\n",
            " 7   PORTFOLIOWEIGHT   120 non-null    float64\n",
            " 8   PRICE             120 non-null    float64\n",
            " 9   ASSETCLASSNAME    120 non-null    object \n",
            " 10  ISSUETYPE         120 non-null    object \n",
            " 11  ISSUECOUNTRYCODE  120 non-null    object \n",
            " 12  ISSUECOUNTRY      120 non-null    object \n",
            " 13  HISTORYDATE       120 non-null    object \n",
            "dtypes: float64(4), object(10)\n",
            "memory usage: 13.3+ KB\n",
            "None\n",
            " df_portfolioperformance:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6 entries, 0 to 5\n",
            "Data columns (total 13 columns):\n",
            " #   Column                    Non-Null Count  Dtype         \n",
            "---  ------                    --------------  -----         \n",
            " 0   PORTFOLIOCODE             6 non-null      object        \n",
            " 1   PORTFOLIOFOCUS            6 non-null      object        \n",
            " 2   AVERAGE_ESG_SCORE         6 non-null      float64       \n",
            " 3   HISTORYDATE               6 non-null      datetime64[ns]\n",
            " 4   CURRENCYCODE              6 non-null      object        \n",
            " 5   CURRENCY                  6 non-null      object        \n",
            " 6   PERFORMANCECATEGORY       6 non-null      object        \n",
            " 7   PERFORMANCECATEGORYNAME   6 non-null      object        \n",
            " 8   PERFORMANCETYPE           6 non-null      object        \n",
            " 9   PERFORMANCEINCEPTIONDATE  6 non-null      datetime64[ns]\n",
            " 10  PORTFOLIOINCEPTIONDATE    6 non-null      datetime64[ns]\n",
            " 11  PERFORMANCEFREQUENCY      6 non-null      object        \n",
            " 12  PERFORMANCEFACTOR         6 non-null      float64       \n",
            "dtypes: datetime64[ns](3), float64(2), object(8)\n",
            "memory usage: 756.0+ bytes\n",
            "None\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HMv5CCePsx5x",
        "ZhbkQe_IKVvb"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}