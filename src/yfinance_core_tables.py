# -*- coding: utf-8 -*-
"""yfinance_core_tables.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kBB_qQuIfSAiPfYUm7aQqaqKyZPzS_C7

#**ESG Equity Fact Sheet - YFinance Data Pipeline**

This notebook extracts and processes real financial data using the Yahoo Finance API (`yfinance`)
and organizes it into structured DataFrames for ESG analysis and fund fact sheet generation.
It produces multiple interrelated tables, covering security metadata, ESG scores, historical prices,
benchmark data, and synthetic portfolio construction. All tables are prepared for Snowflake ingestion,
supporting scalable, enterprise-ready ESG reporting.


###**Execution Instructions**

1. Update your Snowflake credentials in the `.env` file or secure config.
2. Run the notebook sequentially from top to bottom to avoid dependency errors.
3. Use `load_to_snowflake_merge()` for table ingestion to avoid duplication.


### **File Roadmap**
- **Scrape major US index constituents** (S&P 500, NASDAQ 100, Dow Jones)
- **Query Yahoo Finance** for ESG scores and 10 years of historical prices
- **Construct performance and ESG data table** (`df_performance`) with `data_type` flag
- **Generate Security Master** metadata (`df_security_master`)
- **Create synthetic ESG fund holdings** and simulated performance
- **Build benchmark reference tables** using index ETFs (e.g., SHE, ENRG)
- **Validate and inspect all final DataFrames**
- **Prepare all outputs for Snowflake ingestion**

###**Output Summary**

| DataFrame Table Name              | Description                                                  | Source     | Snowflake Table Name                 | Transfer to Snowflake Status |
|----------------------------------|--------------------------------------------------------------|------------|--------------------------------------|-----------------|
| `DF_SECURITY_MASTER`             | Company metadata: sector, industry, market cap               | yFinance   | `SECURITY_MASTER`                    | Completed     |
| `DF_PERFORMANCE`                 | ESG and price data tagged by `data_type`                    | yFinance   | `SECURITY_PERFORMANCE_HISTORY`       | Completed     |
| `DF_HOLDINGSDETAILS`             | Simulated ESG fund holdings: tickers, weights, market value | Synthetic  | `HOLDINGSDETAILS`                    | Not Yet       |
| `DF_PORTFOLIOPERFORMANCE`        | Portfolio-level ESG scores and metadata                      | Synthetic  | `PORTFOLIOPERFORMANCE`               | Not Yet       |
| `DF_BENCHMARK_PERFORMANCE`       | Historical ETF/index prices for benchmarking                | yFinance   | `BENCHMARKPERFORMANCE`               | Not Yet       |
| `DF_BENCHMARK_GENERALINFO`       | Metadata about benchmark indices                             | yFinance   | `BENCHMARKGENERALINFORMATION`        | Not Yet       |
| `DF_BENCHMARK_CHARACTERISTICS`   | Valuation & fundamental metrics (e.g., PE, market cap)       | yFinance   | `BENCHMARKCHARACTERISTICS`           | Not Yet       |


### Next Steps & Future Improvements

#### Security Master + ESG Performance Table
- Add intraday pricing support to better align with real-world execution times
- Introduce dynamic metadata patching if a ticker appears mid-pipeline
- Implement fallback or imputation for missing ESG scores using:
  - Country-level or sector-level averages
- Extend performance history or cache for more robust backfilling
- Improve error handling and retry logic to reduce missing records

#### Synthetic ESG Fund Construction
- Automate creation of portfolios using blended ESG metrics or dynamic clustering
- Introduce ESG attribution to show score drivers at the holding level
- Add time-series ESG tracking to monitor portfolio ESG trend changes
- Load `HOLDINGSDETAILS` and `PORTFOLIOPERFORMANCE` into Snowflake with merge keys
- Evaluate integration of actual fund holdings (if available) for real-world scoring

#### Benchmark Integration
- Create a Snowflake ingestion pipeline for all benchmark DataFrames
- Extend benchmark return metrics to include rolling YTD, 1Y, and 3Y returns
- Create a formal benchmark-to-portfolio mapping table for joins
- Add non-ESG benchmark ETFs (e.g., SPY, ACWI) for relative performance context
- Centralize benchmark definitions to allow scalable mapping across new funds

Each next step aligns with **pipeline scalability**, **fact sheet completeness**, and **production-readiness goals**. Focus areas include:
- More realistic performance simulation
- Robust ESG coverage
- Better join logic for benchmark comparison
- Cleaner, repeatable Snowflake integration

# **Connect to Snowflake**

To load data into Snowflake, we established a secure connection using credentials stored in a .env file. This connection allows us to push data directly from Python. The pipeline is designed to check if tables already exist, create them if needed, and merge new data while avoiding duplicates. This setup enables seamless integration between our local data processing and Snowflake's cloud warehouse, supporting scalable, centralized storage for downstream analytics like ESG reporting and fact sheet generation.
"""

#load the .env file
from google.colab import files
files.upload()

#rename the file if needed
import os

if os.path.exists(".env.txt"):
    os.rename(".env.txt", ".env")
    print("Renamed .env.txt to .env")
else:
    print("File not found. Make sure you uploaded .env.txt.")

!pip install snowflake-connector-python python-dotenv

import os
from dotenv import load_dotenv
import snowflake.connector

# Load .env file data
load_dotenv(".env")

#use .env paramaters to connect to snowflake
def get_snowflake_connection():
    return snowflake.connector.connect(
        user=os.getenv("SNOWFLAKE_USER"),
        password=os.getenv("SNOWFLAKE_PASSWORD"),
        account=os.getenv("SNOWFLAKE_ACCOUNT"),
        role=os.getenv("SNOWFLAKE_ROLE"),
        warehouse=os.getenv("SNOWFLAKE_WAREHOUSE"),
        database=os.getenv("SNOWFLAKE_DATABASE"),
        schema=os.getenv("SNOWFLAKE_SCHEMA")
    )
#connection - connection is authenticated
connection = get_snowflake_connection()
#lets me run SQL commands
cursor = connection.cursor()

from snowflake.connector.pandas_tools import write_pandas

def safe_quote(col: str) -> str:
    """
    Ensures column names are safely quoted for Snowflake SQL syntax.
    Replaces internal quotes and wraps the name in double quotes.
    """
    col = str(col).replace('"', '""').strip()
    return f'"{col}"'

def map_dtype_to_snowflake(dtype):
    """
    Maps pandas dtypes to Snowflake SQL data types.
    """
    if pd.api.types.is_float_dtype(dtype):
        return "FLOAT"
    elif pd.api.types.is_integer_dtype(dtype):
        return "NUMBER"
    else:
        return "VARCHAR"

def load_to_snowflake_merge(df, table_name, conn, unique_keys):
    """
    Uploads DataFrame to Snowflake with type inference and merge logic.
    """
    cur = conn.cursor()
    df_cols = df.columns.tolist()
    temp_table = f"{table_name}_STAGING"

    # Step 1: Infer column types and create table if needed
    col_defs = ", ".join([
        f"{safe_quote(col)} {map_dtype_to_snowflake(df[col].dtype)}"
        for col in df_cols
    ])
    cur.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({col_defs})")

    # Step 2: Add any missing columns to the main table
    cur.execute(f"DESC TABLE {table_name}")
    existing_cols = {row[0].upper() for row in cur.fetchall()}
    for col in df_cols:
        if col.upper() not in existing_cols:
            col_type = map_dtype_to_snowflake(df[col].dtype)
            cur.execute(f"ALTER TABLE {table_name} ADD COLUMN {safe_quote(col)} {col_type}")

    # Step 3: Create staging table
    cur.execute(f"CREATE OR REPLACE TABLE {temp_table} ({col_defs})")
    write_pandas(conn, df, temp_table)

    # Step 4: Merge without duplication
    on_clause = " AND ".join([f"t.{safe_quote(col)} = s.{safe_quote(col)}" for col in unique_keys])
    insert_cols = ", ".join([safe_quote(col) for col in df_cols])
    insert_vals = ", ".join([f"s.{safe_quote(col)}" for col in df_cols])

    merge_stmt = f"""
        MERGE INTO {table_name} t
        USING {temp_table} s
        ON {on_clause}
        WHEN NOT MATCHED THEN
            INSERT ({insert_cols}) VALUES ({insert_vals})
    """
    cur.execute(merge_stmt)

    # Step 5: Clean up
    cur.execute(f"DROP TABLE IF EXISTS {temp_table}")
    cur.close()
    conn.close()

    print(f"{table_name} updated. Duplicates prevented using keys: {unique_keys}")

#test the connection
cursor.execute("SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_DATE;")

for row in cursor:
    print(row)

#close SQL cursor
cursor.close()
#close connection to snowflake
connection.close()

"""# **Securities List**

This code builds a clean and verified list of stocks by scraping 3 major US equity indices from Wikipedia: the S&P500, Dow Jones Industrial Average, and NASDAQ 100. Each of these index lists are retried through BeautifulSoup. The extracted tickers are combined into a single list, cleaned, conform to the expected format, and deduplicated between the 3 indexes. To ensure only valid tickers are included a function was defined to check that Yahoo Finance returns metadata.
<br> <br>
The tickers chosen (S&P500, Dow Jones Industrial Average, NASDAQ 100) represent large, liquid, and well known US companies. They are likely to be included in popular retail and institutional funds, making them a reasonable starting point for building fund simulations and a security master. The decision to limit the scope to these indives was intentional, by focusing on high confdence symbols, the code minimizes errors and avoids excessive querying that could trigger rate limits or bands fron the Yahoo Finance API.
"""

import yfinance as yf
import pandas as pd
import time
from bs4 import BeautifulSoup
import requests

#get tickers from sp500
def get_sp500_tickers():
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    soup = BeautifulSoup(requests.get(url).text, "lxml")
    table = soup.find("table", {"id": "constituents"})
    return [row.find_all("td")[0].text.strip() for row in table.find_all("tr")[1:]]

#get tickers from dow jones indstrial
def get_dow_tickers():
    url = "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"
    soup = BeautifulSoup(requests.get(url).text, "lxml")
    table = soup.find("table", {"id": "constituents"})
    return [
        row.find_all("td")[1].find("a").text.strip()
        for row in table.find_all("tr")[1:]
        if len(row.find_all("td")) >= 2
    ]

#get tickers from NASDAQ-100
def get_nasdaq100_tickers():
    url = "https://en.wikipedia.org/wiki/NASDAQ-100"
    soup = BeautifulSoup(requests.get(url).text, "lxml")
    table = soup.find("table", {"id": "constituents"})
    return [
        row.find_all("td")[0].text.strip()
        for row in table.find_all("tr")[1:]
        if len(row.find_all("td")) >= 1
    ]


#validate that these are real tickers
def is_valid_ticker(ticker):
    try:
        info = yf.Ticker(ticker).info
        return "shortName" in info
    except:
        return False

#pull all tickers from all of the above sources
sp500 = get_sp500_tickers()
dow = get_dow_tickers()
nasdaq = get_nasdaq100_tickers()

#combine and make tickers unique
all_tickers = sorted(set(sp500 + dow + nasdaq))

#data cleaning
all_tickers = [t.replace('.', '-') for t in all_tickers]

"""# **Build Security Master, ESG Scores, and Historical Price Data**

This section constructs and loads three critical datasets using the Yahoo Finance API:

1. **Security Master Table** (`SECURITY_MASTER`): Static metadata including ticker, company name, sector, industry, exchange, currency, and market cap. This serves as the central mapping table for all joins.
2. **Security Performance History** (`SECURITY_PERFORMANCE_HISTORY`): 10 years of daily price data (adjusted close), used to measure fund-level performance over time.
3. **ESG Metrics** (embedded in the performance table): ESG pillar scores (Environmental, Social, Governance), total ESG score, and highest controversy rating are pulled (if available) and stored alongside price history as a separate `data_type = "esg"` entry for each symbol.

###Evolution from Earlier Versions

In prior versions of the pipeline, ESG scores and historical price performance were stored in separate tables (`esg_stock_data` and `stock_performance_history`). While conceptually distinct, both datasets shared common dimensional keys—`symbol` and `date`—and were ultimately used together in downstream fact sheet calculations. As a result, they have been consolidated into a single `df_performance` table with a `data_type` column used to distinguish between `"price"` and `"esg"` entries. This consolidation significantly improves the pipeline’s design by:
- Reducing the number of merge and transformation operations
- Centralizing performance-related metrics into a unified structure
- Enabling more efficient filtering and slicing by symbol, date, and metric type
- Simplifying Snowflake ingestion by requiring fewer staging tables

This shift reflects a deliberate move toward schema scalability, where future metrics such as sentiment scores, news sentiment, or custom factors can be added under the same schema using the same `data_type` strategy.

In addition, several originally included tables were removed from the pipeline:
- **Price Snapshots:** Open, high, low, close summary tables were excluded as their granularity is not utilized in fact sheet metrics, which rely on adjusted close values for performance tracking.
- **Company Fundamentals:** While potentially useful for fundamental analysis or modeling, these were deemed out-of-scope since they are not presented in the public-facing ESG fact sheet.
- **Analyst Estimates:** Similarly, forward-looking target prices or consensus ratings were removed due to their lack of relevance to portfolio ESG reporting.

These removals reduced notebook complexity, shortened runtime, and ensured that all pipeline components directly map to columns or visuals required in the fact sheet. The design philosophy shifted from general-purpose data accumulation to targeted, explainable data extraction that supports a clearly defined deliverable.

###Design Refinements and Lessons Learned

The revised structure reflects several key insights gained during the pipeline’s development:

- Attempting to aggregate all possible financial datasets—fundamentals, analyst views, intraday prices—resulted in a bloated, less maintainable workflow. It became clear that only a subset of data was essential for the ESG fact sheet, and focusing on those core elements yielded a cleaner architecture.
  
- Consolidating ESG and historical performance data into a single table provided multiple advantages:
  - Simplified transformation logic and fewer joins
  - Reduced data fragmentation across Snowflake
  - Better performance when calculating time-based portfolio ESG exposures (e.g., weighted ESG over time)
  - A flexible schema that can be extended using the same `data_type` convention to accommodate future enrichment (e.g., sentiment, climate scores, or controversy alerts)

- Additionally, a better understanding of real-world data pipeline constraints led to critical tradeoffs. For instance:
  - Only end-of-day prices are currently captured, which introduces timing mismatch risk if trades occur intra-day. A future improvement would include intraday pricing or real-time API access to better reflect transaction-level accuracy.
  - The decision to drop fundamentals and analyst data emphasized the importance of scoping to deliverables. Not every dataset, even if useful in isolation, should be included if it doesn't directly impact outputs or adds maintenance overhead.

- The project now prioritizes a **targeted, modular, and scalable** approach where each table and script is clearly aligned with a downstream analytical need.

These refinements not only enhance technical maintainability but also improve traceability, documentation quality, and the ability for others to extend the work—all critical components of a production-grade analytics pipeline.

###Processing Logic Summary

- Each ticker is queried for metadata and stored in `security_master`
- 10 years of daily price history is collected for performance tracking
- ESG data is fetched as a one-time snapshot (if available), transposed, and tagged into the performance table
- All failed tickers (e.g., invalid or delisted) are logged separately

> Note: To avoid API rate limits, a 1-second pause is included after each ticker fetch. This section takes ~14 minutes to complete depending on the size of the ticker list.

###Future Improvements

- **Intraday Price Collection**: Currently, price performance is only tracked using end-of-day (EOD) prices. In real-world trading, the time of trade execution can materially affect performance. A future upgrade would involve capturing intraday prices (e.g., hourly or timestamped events) to better align with the actual time of purchase and generate more accurate return calculations.
- **Auto-Patching Missing Metadata**: Future versions should detect if a symbol is missing from the security master (e.g., if it appears in holdings or ESG) and automatically fetch the missing metadata dynamically.
- **Handling Missing ESG Scores**: If ESG data is unavailable for a given ticker, the system could impute values using:
  - Country-level ESG averages (based on country code)
  - Peer group or industry-level averages (using sector or industry classification)
  This would maintain completeness for fund-level ESG calculations while minimizing the impact of sparse coverage.
- **Historical Data Expansion**: The 10-year window was chosen to balance completeness and API reliability. Long-term, the pipeline could be enhanced to:
  - Refresh historical prices periodically
  - Backfill longer histories using a persistent cache or separate update job
- **Retry Logic and Logging**: More robust error handling could help avoid incomplete datasets when a transient API failure occurs.

###Snowflake Output Mapping

| Table Name                     | Description                                      | Unique Keys            |
|-------------------------------|--------------------------------------------------|------------------------|
| `SECURITY_MASTER`             | Static security-level reference data             | `ticker`               |
| `SECURITY_PERFORMANCE_HISTORY`| Daily prices + ESG scores (via `data_type`)     | `ticker`, `Date`       |
"""

#14 mins to run
import yfinance as yf
import datetime
import pandas as pd
import time

# Initialize lists
security_master = []
combined_data = []
failed_tickers = []

# --- Date range for historical price data (last 10 years) ---
start_date = (datetime.datetime.today() - datetime.timedelta(days=365 * 10)).strftime('%Y-%m-%d')
end_date = datetime.datetime.today().strftime('%Y-%m-%d')

for symbol in all_tickers:
    try:
        t = yf.Ticker(symbol)
        info = t.info

        # Skip invalid tickers
        if not info or "shortName" not in info:
            print(f"No valid info for {symbol}")
            failed_tickers.append(symbol)
            continue

        # --- Security Master ---
        security_master.append({
            "ticker": symbol,
            "shortName": info.get("shortName"),
            "name": info.get("longName"),
            "sector": info.get("sector"),
            "industry": info.get("industry"),
            "exchange": info.get("exchange"),
            "currency": info.get("currency"),
            "country": info.get("country"),
            "market_cap": info.get("marketCap")
        })

        # --- Historical Performance ---
        hist = t.history(start=start_date, end=end_date)
        if hist.empty:
            print(f"No price history for {symbol}")
            failed_tickers.append(symbol)
            continue

        hist = hist.reset_index()
        hist["ticker"] = symbol
        hist["data_type"] = "price"
        combined_data.append(hist)

        # --- ESG as single-row record ---
        sustainability = t.sustainability
        if sustainability is not None and not sustainability.empty:
            row = sustainability.transpose()
            esg_row = {
                "Date": pd.to_datetime("today"),
                "Open": None,
                "High": None,
                "Low": None,
                "Close": None,
                "Volume": None,
                "Dividends": None,
                "Stock Splits": None,
                "ticker": symbol,
                "data_type": "esg",
                "esgPerformance": row.get("esgPerformance", {}).values[0] if "esgPerformance" in row else None,
                "totalEsg": row.get("totalEsg", {}).values[0] if "totalEsg" in row else None,
                "environmentScore": row.get("environmentScore", {}).values[0] if "environmentScore" in row else None,
                "socialScore": row.get("socialScore", {}).values[0] if "socialScore" in row else None,
                "governanceScore": row.get("governanceScore", {}).values[0] if "governanceScore" in row else None,
                "highestControversy": row.get("highestControversy", {}).values[0] if "highestControversy" in row else None
            }
            combined_data.append(pd.DataFrame([esg_row]))

        # Sleep to avoid hitting API rate limits
        time.sleep(1)

    except Exception as e:
        print(f"Error with {symbol}: {e}")
        failed_tickers.append(symbol)

# Convert to DataFrames
df_security_master = pd.DataFrame(security_master)
df_performance = pd.concat(combined_data, ignore_index=True) if combined_data else pd.DataFrame()

df_performance["Date"] = pd.to_datetime(df_performance["Date"], utc=True).dt.tz_localize(None).dt.date


# --- Output summary ---
print(f"\n Finished processing {len(all_tickers)} tickers.")
print(f" Failed tickers: {len(failed_tickers)}")
print(failed_tickers)

#connect to snowflake and load the data directly bypassing any previously loaded data
conn = get_snowflake_connection()
load_to_snowflake_merge(df_security_master, "SECURITY_MASTER", conn, unique_keys=["ticker"])

conn = get_snowflake_connection()
if not df_performance.empty:
    load_to_snowflake_merge(df_performance, "SECURITY_PERFORMANCE_HISTORY", conn, unique_keys=["Date", "ticker"])

"""# **Generate Synthetic ESG Fund Holdings and Portfolio Performance**

This section simulates synthetic ESG-focused funds by programmatically selecting top companies based on ESG metrics and assigning fund weights. It produces two key downstream tables for the ESG fact sheet:

1. **HoldingsDetails Table** (`df_holdingsdetails`): Represents the portfolio composition of each synthetic ESG fund, including ticker-level weights, market values, quantities, sector and industry classification, and region-level attributes.
2. **PortfolioPerformance Table** (`df_portfolioperformance`): Stores metadata and synthetic performance metrics per fund, incorporating ESG-specific focus areas and scoring.

These tables follow the structure of existing **Assette benchmark tables** and were recreated to be **appended to Snowflake** in later stages of the project.  
- For `HOLDINGSDETAILS`, only a subset of the full schema was used — the most relevant columns were selected to support reporting needs.  
- For `PORTFOLIOPERFORMANCE`, additional metadata columns were introduced (e.g., `AVERAGE_ESG_SCORE`, `PORTFOLIOFOCUS`) as defined in the `portfolioperformance.py` module to enrich downstream fact sheet rendering.


###Processing Logic

- ESG and price data are first separated from the consolidated performance table (`df_performance`) using the `data_type` flag.
- Only the most recent ESG and price observations for each valid ticker (based on `security_master`) are retained using `drop_duplicates(...)` logic sorted by date.
- A list of synthetic funds is defined, each with a specific ESG focus (e.g., `environmentScore`, `socialScore`, `highestControversy`).
- For each fund:
  - Companies are ranked based on the relevant ESG metric (lowest scores are better, except for `highestControversy`).
  - The top 20 names with valid scores and prices are selected.
  - Random weights are generated and normalized to simulate realistic fund allocation.
  - These weights are applied to calculate total market value and shares held using a fixed $100M notional.

The result is a simulated portfolio for each fund that reflects its ESG investment mandate.


###Output Tables

#### `df_holdingsdetails`
Includes key fund composition attributes such as:
- `PORTFOLIOCODE`, `TICKER`, `MARKETVALUE`, `QUANTITY`, `PORTFOLIOWEIGHT`
- `PRICE`, `ASSETCLASSNAME`, `ISSUETYPE`, `ISSUECOUNTRY`

> *Note: Only a subset of fields from the full `HOLDINGSDETAILS` schema was used. The selected columns were chosen to support fact sheet requirements and streamline reporting.*

#### `df_portfolioperformance`
Captures synthetic performance and fund-level ESG insights, including:
- `PORTFOLIOFOCUS`: The ESG dimension used to construct the portfolio
- `AVERAGE_ESG_SCORE`: The best ESG score among holdings (min or max depending on score type)
- `PERFORMANCEFACTOR`: Simulated gross return for illustrative purposes
- Standard portfolio metadata (inception dates, currency, frequency, etc.)
- Additional fields defined in `portfolioperformance.py` for compatibility with Assette’s Snowflake structure



###Additions Compared to Previous Versions

- ESG scores were previously stored separately from performance data; now, a unified performance table supports both metrics and simplifies filtering.
- The ESG average score is now stored directly in the performance table for each portfolio (`AVERAGE_ESG_SCORE`), enhancing downstream fact sheet rendering.
- Several metadata columns (e.g., `PERFORMANCEFREQUENCY`, `PORTFOLIOFOCUS`) were added to improve interpretability and meet fact sheet requirements.



###Future Improvements

- **Dynamic ESG Blending**: Instead of selecting securities based on a single ESG dimension (e.g., `environmentScore`), future versions could use a weighted or blended ESG score across multiple dimensions to simulate more sophisticated fund strategies.

- **ESG Attribution Analysis**: A separate attribution module could help explain which securities drive positive or negative changes in fund-level ESG scores, adding transparency to synthetic fund construction.

- **Time-Series ESG Trends**: Extend ESG scoring to a time series format to enable fund-level ESG trend tracking (e.g., quarterly improvements in environmental impact).

- **Scalable Fund Definitions**: Instead of manually defining six portfolios, future versions could dynamically generate portfolios by theme, region, or sector using metadata and clustering techniques.

- **Integration of Real Holdings Data**: While this section currently generates synthetic holdings, future integration with actual fund holdings (if available) could enable ESG scoring of real portfolios and comparison to synthetic benchmarks.

- **Snowflake Integration for Synthetic Tables**: As of now, `HOLDINGSDETAILS` and `PORTFOLIOPERFORMANCE` are generated locally in Python but are **not yet connected to Snowflake**. A key next step would be to define and create corresponding Snowflake tables, and use a `load_to_snowflake_merge()` function to push the data, using composite keys to avoid duplication and maintain referential integrity.

###DataFrame Output Mapping

| Table Name              | Description                                                  | Unique Keys                             |
|------------------------|--------------------------------------------------------------|------------------------------------------|
| `DF_HOLDINGSDETAILS`      | Simulated fund holdings: tickers, weights, market values     | `PORTFOLIOCODE`, `TICKER`, `HISTORYDATE` |
| `DF_PORTFOLIOPERFORMANCE` | Synthetic fund-level ESG scores and performance metrics       | `PORTFOLIOCODE`, `HISTORYDATE`           |
"""

import pandas as pd
import numpy as np
from datetime import datetime

# Normalize 'Date' column
df_performance.rename(columns=lambda x: x.strip().capitalize() if x.lower() == 'date' else x, inplace=True)

# Step 1: Filter valid symbols
valid_symbols = df_security_master['ticker'].unique()

# Step 2: Extract ESG and price data
df_esg = df_performance[df_performance['data_type'] == 'esg'].copy()
df_esg = df_esg[df_esg['ticker'].isin(valid_symbols)]
df_esg_latest = df_esg.sort_values('Date').drop_duplicates('ticker', keep='last')

df_price = df_performance[df_performance['data_type'] == 'price'].copy()
df_price = df_price[df_price['ticker'].isin(valid_symbols)]
df_price_latest = df_price.sort_values('Date').drop_duplicates('ticker', keep='last')
df_price_latest = df_price_latest[['ticker', 'Date', 'Close']].rename(columns={'Close': 'price'})

# Step 3: Define synthetic funds
funds = pd.DataFrame({
    "PORTFOLIOCODE": [
        "Climate_Leaders_Fund", "Social_Impact_Fund", "Governance_Focused_Fund",
        "Low_Controversy_Fund", "Overall_ESG_Leaders", "Stakeholder_Advocacy_Fund"
    ],
    "PORTFOLIOFOCUS": [
        "environmentScore", "socialScore", "governanceScore",
        "highestControversy", "totalEsg", "socialScore"
    ]
})

# Step 4: Generate holdingsdetails table
fund_value_usd = 100_000_000
holdings_rows = []

for _, fund in funds.iterrows():
    focus = fund["PORTFOLIOFOCUS"]
    df = df_esg_latest.copy()

    # Correct sorting logic: lower score = better, except highestControversy
    df = df.sort_values(focus, ascending=(focus != 'highestControversy')).dropna(subset=[focus]).head(20).copy()


    df["raw_weight"] = np.abs(np.random.rand(len(df)))
    df = df.merge(df_price_latest, on='ticker', how='left').dropna(subset=["price"])
    df["weight"] = df["raw_weight"] / df["raw_weight"].sum()


    df_meta = df.merge(df_security_master, on='ticker', how='left')

    for _, row in df_meta.iterrows():
        invested = fund_value_usd * row["weight"]
        shares = invested / row["price"]
        holdings_rows.append({
            "PORTFOLIOCODE": fund["PORTFOLIOCODE"],
            "CURRENCYCODE": row["currency"],
            "CURRENCY": "US Dollar",
            "ISSUENAME": row["name"],
            "TICKER": row["ticker"],
            "QUANTITY": round(shares, 2),
            "MARKETVALUE": round(invested, 2),
            "PORTFOLIOWEIGHT": round(row["weight"] , 6),
            "PRICE": round(row["price"], 2),
            "ASSETCLASSNAME": row["sector"],
            "ISSUETYPE": row["industry"],
            "ISSUECOUNTRYCODE": row["exchange"],
            "ISSUECOUNTRY": row["country"],
            "HISTORYDATE": row["Date_x"]
        })

df_holdingsdetails = pd.DataFrame(holdings_rows)

# Step 5: Create portfolioperformance table
performance_date = pd.to_datetime("2025-06-30")
inception_date = pd.to_datetime("2023-01-01")

# Compute ESG averages based on fund_focus
avg_scores = []
for _, fund in funds.iterrows():
    focus = fund["PORTFOLIOFOCUS"]
    symbols = df_holdingsdetails[df_holdingsdetails["PORTFOLIOCODE"] == fund["PORTFOLIOCODE"]]["TICKER"]
    values = df_esg_latest[df_esg_latest["ticker"].isin(symbols)][focus]

    if focus == "highestControversy":
        avg_score = round(values.max(), 2)  # higher is better
    else:
        avg_score = round(values.min(), 2)  # lower is better

    avg_scores.append(avg_score)

funds["AVERAGE_ESG_SCORE"] = avg_scores

df_portfolioperformance = funds.assign(
    HISTORYDATE=performance_date,
    CURRENCYCODE="USD",
    CURRENCY="US Dollar",
    PERFORMANCECATEGORY="Asset Class",
    PERFORMANCECATEGORYNAME="Total Portfolio",
    PERFORMANCETYPE="Portfolio Gross",
    PERFORMANCEINCEPTIONDATE=inception_date,
    PORTFOLIOINCEPTIONDATE=inception_date,
    PERFORMANCEFREQUENCY="D",
    PERFORMANCEFACTOR=np.round(np.random.normal(loc=0.001, scale=0.01, size=len(funds)), 6)
)

# --- OUTPUT ---
print(" HoldingsDetails:")
print(df_holdingsdetails.head())

print("\n PortfolioPerformance:")
print(df_portfolioperformance.head())

"""# **Benchmark Selection and ESG Performance Construction**

This section simulates benchmark datasets by extracting real market data and formatting it to align with Assette’s schema requirements.
Since ESG scores are not available for index tickers via Yahoo Finance, the section focuses exclusively on price-based performance, general metadata, and numeric characteristics.
It produces three key downstream tables for the ESG fact sheet:

1. **BenchmarkPerformance Table** (`df_benchmark_performance`): Historical price data from Yahoo Finance is reformatted to mirror Assette’s performance structure, supporting consistent return calculations across funds and benchmarks.

2. **BenchmarkGeneralInformation Table** (`df_benchmark_general_info`): Stores metadata such as ticker, name, and asset class context. This ensures compatibility with Assette’s schema and allows for accurate labeling and reporting.

3. **BenchmarkCharacteristics Table** (`df_benchmark_characteristics`): Captures numeric and descriptive company-level characteristics (e.g., market cap, P/E ratio), which are relevant for fund comparison and analytical filtering in downstream tools.

The resulting DataFrames follow the structure of existing **Assette benchmark tables** and were recreated to be **appended to Snowflake** in later stages of the project.

- For `df_benchmark_performance`, historical price data was formatted to match Assette’s performance table, enabling consistent return tracking across funds and benchmarks.  
- For `df_benchmark_general_info`, metadata fields such as ticker symbol, display name, and performance context were aligned with expected schema formats.  
 For `df_benchmark_characteristics`, all available numeric descriptors (e.g., market cap, PE ratio) were extracted and structured for compatibility with Assette’s benchmark characteristics schema.

These tables are critical for enabling **performance comparison and benchmarking** of the synthetic ESG portfolios. Without them, it would be difficult to contextualize fund performance or validate ESG-driven investment logic against broader market indices.

###Benchmark Selection Logic

Each benchmark ticker is manually mapped to a specific ESG focus area based on the theme of the corresponding synthetic fund. These ETFs are selected because they reflect real-world ESG-aligned investment strategies. While direct ESG scores are not available for indices, these benchmarks are broadly representative of their respective ESG dimensions.

| `PORTFOLIOCODE`               | `PORTFOLIOFOCUS`      | Mapped Benchmark | Rationale |
|------------------------------|------------------------|------------------|-----------|
| `Climate_Leaders_Fund`       | `environmentScore`     | `ENRG`           | Focuses on clean energy and environmental sustainability |
| `Social_Impact_Fund`         | `socialScore`          | `SHE`            | Promotes gender diversity and social equity |
| `Governance_Focused_Fund`    | `governanceScore`      | `VOTE`           | Targets corporate governance and shareholder rights |
| `Low_Controversy_Fund`       | `highestControversy`   | `ESGD`           | General ESG index used in absence of controversy-specific benchmarks |
| `Overall_ESG_Leaders`        | `totalEsg`             | `EFIV`           | U.S. ESG leaders index for overall ESG exposure |
| `Stakeholder_Advocacy_Fund`  | `socialScore`          | `SHE` (again)    | Reuses `SHE` for its alignment with stakeholder and advocacy themes |

> Note: Some benchmarks (like `SHE`) are reused across portfolios with similar ESG focus areas due to the limited number of available ESG-themed ETFs. A dedicated mapping table can be introduced for automation and scalability.


###Outputs Tables

This section produces three structured DataFrames that are aligned with existing Assette benchmark tables:

####**`df_benchmark_performance`**
   - Daily close price history over the past 10 years
   - Includes fields like `PERFORMANCETYPE`, `CURRENCYCODE`, and `HISTORYDATE`
   - Used to support time-series benchmark comparison with synthetic fund performance

####**`df_benchmark_general_info`**
   - Basic metadata per benchmark, including ticker symbol, benchmark name, and performance logic
   - Used as a header or reference table in reporting layers and fact sheet displays

####**`df_benchmark_characteristics`**
   - Captures all numeric fields from the benchmark’s `.info` object (e.g., market cap, PE ratio, beta)
   - Enhances analytical context and allows high-level comparisons across benchmarks and funds

These tables are important because they provide **benchmark context for ESG portfolio returns**. Without benchmark comparators, it’s difficult to assess relative performance or risk-adjusted return. They serve as the anchor for comparison in the fact sheet and allow validation of fund construction logic.

#### Notes

- Only tickers that return both `.info` metadata and valid historical `.history` data are included.
- Currency defaults to `USD` when missing, ensuring consistency across benchmarks.
- Both `HISTORYDATE` and `HISTORYDATE1` are preserved for compatibility with different reporting tools or date formats.


### Future Improvements

- **Snowflake Integration**: These benchmark datasets are currently only created in Python. A next step will be to **append them to Assette’s Snowflake database** using `MERGE` statements and defined unique keys, aligning them with the holdings and portfolio tables already integrated.

- **Benchmark-to-Portfolio Mapping Table**: Introduce a structured mapping table that formally links each synthetic portfolio to its chosen benchmark. This will allow easier joins and automated comparisons in fact sheet generation.

- **Benchmark Return Aggregation**: Extend the current raw price history with calculated rolling return metrics (e.g., YTD, 1Y, 3Y) to match fund performance summaries in the fact sheet.

- **Expanded Benchmark Coverage**: Add non-ESG reference indices (e.g., `SPY`, `ACWI`, `MSCI World`) to provide broader context and allow performance benchmarking beyond ESG-only strategies.
- **Expanded Benchmark Coverage**: Add non-ESG reference indices (e.g., `SPY`, `ACWI`, `MSCI World`) to provide broader context and allow performance benchmarking beyond ESG-only strategies.

###DataFrame Output Mapping

| Table Name                 | Description                                                               | Unique Keys                        |
|---------------------------|---------------------------------------------------------------------------|------------------------------------|
| DF_BENCHMARK_PERFORMANCE      | Historical benchmark prices and returns                                   | SYMBOL, HISTORYDATE                |
| DF_BENCHMARK_GENERALINFO | Benchmark metadata: name, display name, region, asset class               | SYMBOL                             |
| DF_BENCHMARK_CHARACTERISTICS  | Descriptive stats like market cap, PE ratio, dividend yield, etc.         | SYMBOL, HISTORYDATE                |
"""

import yfinance as yf
import pandas as pd
import datetime

# Define benchmark tickers
benchmark_tickers = ["ENRG", "SHE", "VOTE", "ESGD", "EFIV"]

# Define date range (last 3 years)
start_date = datetime.datetime.today() - datetime.timedelta(days=365 * 10)
end_date = datetime.datetime.today()

# Initialize containers
benchmark_performance = []
benchmark_general_information = []
benchmark_characteristics = []

# Pull data for each benchmark
for ticker in benchmark_tickers:
    try:
        t = yf.Ticker(ticker)
        info = t.info
        hist = t.history(start=start_date, end=end_date)

        if hist.empty or not info:
            continue

        # === Benchmark Performance ===
        hist = hist.reset_index()
        df_perf = pd.DataFrame({
            "BENCHMARKCODE": ticker.lower(),
            "PERFORMANCETYPE": "Prices",
            "CURRENCYCODE": info.get("currency", "USD"),
            "CURRENCY": info.get("financialCurrency", info.get("currency", "USD")),
            "PERFORMANCEFREQUENCY": "Daily",
            "VALUE": hist["Close"],
            "HISTORYDATE1": hist["Date"].dt.date,
            "HISTORYDATE": hist["Date"]
        })
        benchmark_performance.append(df_perf)

        # === Benchmark General Information ===
        benchmark_general_information.append({
            "BENCHMARKCODE": ticker.lower(),
            "TICKER": ticker,
            "NAME": info.get("longName", info.get("shortName", ticker)),
            "ISBEGINOFDAYPERFORMANCE": False
        })

        # === Benchmark Characteristics (auto-detect numeric fields) ===
        for key, value in info.items():
            if isinstance(value, (int, float)):
                benchmark_characteristics.append({
                    "BENCHMARKCODE": ticker.lower(),
                    "CURRENCYCODE": info.get("currency", "USD"),
                    "CURRENCY": info.get("financialCurrency", info.get("currency", "USD")),
                    "LANGUAGECODE": "en-US",
                    "CATEGORY": "Total",
                    "CATEGORYNAME": None,
                    "CHARACTERISTICNAME": key,
                    "CHARACTERISTICDISPLAYNAME": key.replace('_', ' ').title(),
                    "STATISTICTYPE": "NA",
                    "CHARACTERISTICVALUE": value,
                    "ABBREVIATEDTEXT": None,
                    "HISTORYDATE": datetime.date.today()
                })

    except Exception as e:
        print(f" Error with {ticker}: {e}")

# Create DataFrames
df_benchmark_performance = pd.concat(benchmark_performance, ignore_index=True)
df_benchmark_general_info = pd.DataFrame(benchmark_general_information)
df_benchmark_characteristics = pd.DataFrame(benchmark_characteristics)

# Preview
print(" Performance sample:")
print(df_benchmark_performance.head(3))
print("\n General Info sample:")
print(df_benchmark_general_info.head(3))
print("\n Characteristics sample:")
print(df_benchmark_characteristics.head(3))

"""# **DataFrame Summary**

This section provides a final integrity check across all major output DataFrames.
It prints schema summaries (`.info()`) to validate structure, data types, and null counts
prior to Snowflake ingestion or downstream fact sheet rendering.

The following DataFrames are printed:

- **df_security_master:**        Source metadata table for ticker symbols and names.
- **df_performance:**            ESG and price data (raw + transformed) from yFinance and synthetic sources.
- **df_holdingsdetails:**        Synthetic fund holdings (ticker weights, values, sector info).
- **df_portfolioperformance:**   Synthetic portfolio-level ESG scores and metadata.
- **df_benchmark_performance:**  Normalized historical prices and calculated returns for selected benchmarks.
- **df_benchmark_general_info:** Ticker-level metadata fields for each benchmark index.
- **df_benchmark_characteristics:** Quantitative descriptors (e.g., market cap, PE) for benchmark indices.
"""

print(" df_security_master:")
print(df_security_master.info())
print(" df_performance:")
print(df_performance.info())
print(" df_benchmark_performance:")
print(df_benchmark_performance.info())
print(" df_benchmark_general_info:")
print(df_benchmark_general_info.info())
print(" df_benchmark_characteristics:")
print(df_benchmark_characteristics.info())
print(" df_holdingsdetails:")
print(df_holdingsdetails.info())
print(" df_portfolioperformance:")
print(df_portfolioperformance.info())
