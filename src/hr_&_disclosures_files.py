# -*- coding: utf-8 -*-
"""hr_&_disclosures_files.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12OFdR7Bawu37JGK7reMh257pk3VUuP-o

# **HR & Disclosures Synthetically Generated Data**

This notebook generates synthetic data needed for ESG equity fund fact sheets, specifically for the team summary and compliance disclosures sections.

- For HR, synthetic data simulates what would be manually provided by the HR department (as per business advisor guidance). This includes names, titles, tenure, and team affiliations. The data is output as a CSV and prepared for GPT-based summary generation to describe the team managing each fund.

- For compliance, a reusable list of regulatory disclosure footnotes has been synthetically created. These footnotes are tagged with unique IDs and labeled as either mandatory or optional. For each fund, the compliance team will provide a list of relevant disclosure IDs, which are then pulled and added to the fund's fact sheet.


###**Execution Instructions**

**To run this notebook:**
1. Run the notebook sequentially from top to bottom to generate and export HR and Compliance data
2. Output files will be saved locally and visible in the file panel

### **File Roadmap**
HR Data: Faker-generated employee names, titles, tenure, and team <br>
Compliance Footnotes: Reusable disclosures labeled with DisclosureID and category <br>
**Outputs:** synthetic_employee_hr.csv, full_compliance_footnotes.csv
"""

pip install faker

"""# **HR**

HR data for this project is synthetically created to simulate how team information would be incorporated into ESG fund fact sheets. As discussed with the business advisor, real HR data will be provided manually since the HR team operates within separate systems. To emulate this, we generated synthetic HR profiles, including names, roles, tenure, and team affiliations, and saved them to a CSV file. This dataset serves as input for the GPT API, which will be used to generate narrative summaries about the fund teams responsible for managing each product. These summaries are intended to provide investors with a concise, professional overview of the team behind their fund. While the data generation and CSV preparation are complete, the next step is to generate the GPT-based summaries. The final deliverable will be a table containing the fund name and corresponding team summary, ready to be manually inserted into the final fact sheets.

#### **HR Data Pipeline**
1. Synthetic HR Data Creation - For development and demonstration purposes, we synthetically generated HR data including names, titles, tenure, and teams.
2. CSV Output- The synthetic HR data is saved as a CSV file to mirror how the real HR data might be delivered manually.
3. GPT-Based Summary Generation (Pending) - A GPT function will read the HR CSV and generate a narrative team summary for each fund, describing who manages the fund and their qualifications.
4. Manual Transfer Step - Once the GPT summaries are generated, they will be shared with the business team to be inserted into the final fact sheets manually.

#### **Final Output Format** : CSV/Excel : fund_name, team_summary

## **Generate Synthetic HR data**

The script generates synthetic HR data tailored for an asset management firm by creating fake employee records with realistic fields such as name, start date, job function, title, team, education, and key personnel status using the Faker library. It calculates industry experience based on the start date, saves the dataset to a CSV file, and includes a separate function to load and clean this data for analysis, standardizing column names and recalculating experience if needed.
"""

import pandas as pd
import random
from faker import Faker
from datetime import datetime

fake = Faker()

def generate_synthetic_hr_data(num_records=20, output_path='synthetic_employee_hr.csv'):
    """
    Generates synthetic HR data for asset management roles and saves as CSV.
    """
    job_functions = [
        'Equity Research Analyst', 'Fixed Income Analyst', 'Quantitative Analyst',
        'Portfolio Manager', 'Research Associate', 'Investment Strategist',
        'ESG Analyst', 'Trader', 'Risk Analyst', 'Compliance Officer',
        'Operations Manager', 'Chief Investment Officer (CIO)', 'Product Specialist'
    ]
    titles = ['Analyst', 'Associate', 'Vice President', 'Director', 'Managing Director']
    teams = ['Equities', 'Fixed Income', 'Multi-Asset', 'Quant', 'Compliance']
    education_levels = ['MBA', 'CFA', 'PhD', 'BBA', 'MS Finance', 'MA Econ']
    employment_types = ['Full-time', 'Contract']
    locations = ['New York', 'Boston', 'London', 'San Francisco', 'Chicago']

    data = []
    for i in range(num_records):
        start_date = fake.date_between(start_date='-15y', end_date='-1y')
        years_exp = round((datetime.today().date() - start_date).days / 365.25, 1)
        job_function = random.choice(job_functions)
        name = fake.name()
        employee_id = f"EID{i+1000}"
        title = random.choice(titles)
        team = random.choice(teams)
        education = random.choice(education_levels)
        employment_type = random.choice(employment_types)
        location = random.choice(locations)
        is_key_personnel = random.choices([True, False], weights=[0.3, 0.7])[0]
        status = random.choices(['Active', 'Inactive'], weights=[0.85, 0.15])[0]

        data.append({
            'EmployeeID': employee_id,
            'Name': name,
            'StartDate': start_date,
            'IndustryExperienceYears': years_exp,
            'PrimaryJobFunction': job_function,
            'Title': title,
            'Team': team,
            'Education': education,
            'EmploymentType': employment_type,
            'Location': location,
            'IsKeyPersonnel': is_key_personnel,
            'Status': status
        })

    df_hr = pd.DataFrame(data)
    df_hr.to_csv(output_path, index=False)
    print(f"Synthetic HR data saved to: {output_path}")
    return df_hr

def load_hr_data(file_path):
    """
    Loads and cleans HR data CSV. Calculates years of experience if not provided.
    """
    df_hr = pd.read_csv(file_path, parse_dates=['StartDate'])

    # Normalize column names
    df_hr.columns = df_hr.columns.str.strip().str.lower()

    # Rename for clarity
    df_hr.rename(columns={
        'startdate': 'start_date',
        'industryexperienceyears': 'industry_experience_years',
        'primaryjobfunction': 'job_function'
    }, inplace=True)

    # Calculate experience if missing
    if 'industry_experience_years' not in df_hr.columns or df_hr['industry_experience_years'].isnull().all():
        df_hr['industry_experience_years'] = (datetime.today() - df_hr['start_date']).dt.days / 365.25

    return df_hr

# Example run
if __name__ == "__main__":
    generate_synthetic_hr_data(50, 'synthetic_employee_hr.csv')
    df_hr = load_hr_data('synthetic_employee_hr.csv')
    print(df_hr.head())

"""## **Load HR Data**

This updated load_hr_data() function reads the synthetic HR CSV file, standardizes column names, parses the StartDate, and ensures consistency across key fields. It recalculates industry experience from the start date if the value is missing or incorrect and rounds it to one decimal place. The function also converts specific columns like IsKeyPersonnel, Status, EmploymentType, and Team into appropriate data types such as booleans and categories, making the dataset cleaner and more efficient for analysis or visualization in dashboards or reporting tools.
"""

import pandas as pd
from datetime import datetime

def load_hr_data(file_path):
    """
    Loads and cleans HR data from CSV.
    Standardizes column names, parses dates, and verifies key fields.
    """
    df_hr = pd.read_csv(file_path, parse_dates=['StartDate'])

    # Normalize column names
    df_hr.columns = df_hr.columns.str.strip().str.lower()

    # Rename key columns for consistency
    df_hr.rename(columns={
        'startdate': 'start_date',
        'industryexperienceyears': 'industry_experience_years',
        'primaryjobfunction': 'job_function'
    }, inplace=True)

    # Recalculate experience if missing or corrupted
    if 'industry_experience_years' not in df_hr.columns or df_hr['industry_experience_years'].isnull().any():
        df_hr['industry_experience_years'] = (datetime.today() - df_hr['start_date']).dt.days / 365.25
        df_hr['industry_experience_years'] = df_hr['industry_experience_years'].round(1)

    # Optional: Convert booleans & categories
    df_hr['iskeypersonnel'] = df_hr['iskeypersonnel'].astype(bool)
    df_hr['status'] = df_hr['status'].astype('category')
    df_hr['employmenttype'] = df_hr['employmenttype'].astype('category')
    df_hr['team'] = df_hr['team'].astype('category')
    df_hr['job_function'] = df_hr['job_function'].astype('category')

    return df_hr

# Example usage
if __name__ == "__main__":
    hr_df = load_hr_data('synthetic_employee_hr.csv')
    print(hr_df.head())

df_hr.info()

"""## **GPT API for Team Summary**"""

#next steps - reach out to jacob regarding this portion. Maybe we can add this to Jacob's GPT sheet or do it in this sheet.

"""# **Compliance & Disclosure Information**

Compliance and disclosure information for fund fact sheets is managed through a centralized, reusable repository. As advised by the business team, most disclosures are consistent across funds and can be reused as needed. To support this, we generated a synthetic master list of disclosures, each labeled with a unique DisclosureID, its description, and a tag indicating whether it is mandatory or optional. For each fund, the compliance team will provide the specific IDs applicable to that product, and those entries will be pulled from the master list and added to the fact sheet. This approach allows for consistency, ease of updates, and simplified integration with downstream reporting. The final output will be a table of selected disclosures that align with each fund’s regulatory and marketing requirements.

#### **Compliance & Disclosure Pipeline**
1. Synthetic Disclosure List Creation - A synthetic set of compliance footnotes has been generated, each with a unique DisclosureID and corresponding description text. Each record is tagged as mandatory or optional.
2. Central Repository Approach - This disclosure list will act as a central, master repository. It can be maintained and updated by the compliance team outside of the pipeline (in Excel or Snowflake).
3. Manual Input from Compliance - For each fact sheet, the compliance team will specify which DisclosureIDs apply to that fund. These IDs will be matched to the master list and pulled into the fact sheet.
4. Fund-Specific Disclosure DataFrame Generation (Pending)- Load the provided DisclosureIDs for a specific fund from compliance, pull the matching footnotes from the master list, and generate a structured table of disclosures.
5. Fund-Specific Disclosure Table Generation (Future Improvement) - Previous step's table will  be loaded into Snowflake for use in final fact sheet assembly.


#### **Final Output Format** : CSV/Excel: fund_name, footnote

## **Synthetic Compliance & Disclosure Information**

This function creates a synthetic dataset of compliance footnotes and disclosures commonly found in ESG equity fact sheets. Each record includes a disclosure ID, topic, descriptive footnote, effective date, disclosure type (e.g., legal, performance), applicable entity (e.g., fund or strategy), review details, and a DisclosureRequirement field that flags whether the disclosure is mandatory or voluntary. The requirement is determined based on the disclosure type, with legal and regulatory types defaulting to mandatory. The final dataset is saved as a CSV file for analysis or reporting.
"""

def generate_full_compliance_data(num_records=25, output_path='full_compliance_footnotes.csv'):
    """
    Generates synthetic compliance footnotes and disclosures for ESG equity fact sheets,
    incorporating regulatory disclaimers, usage restrictions, and disclosure classifications.
    """
    topics = [
        "Risk Disclosure", "ESG Methodology", "Benchmark Comparison",
        "Performance Past vs Future", "Index Usage", "Data Source",
        "Carbon Footprint", "Proxy Voting", "Engagement Policy",
        "Sustainable Investing Risk", "Regulatory Classification",
        "Holdings Disclosure", "Sector Allocation Methodology",
        "Data Accuracy Disclaimer", "Third-Party Content Disclaimer",
        "Marketing Classification", "Jurisdictional Disclosure",
        "Investor Rights", "Professional Use Only", "Fund Documentation Reference",
        "Source Attribution", "Reproduction Restriction"
    ]

    footnotes = {
        "Risk Disclosure": "Past performance is not indicative of future results.",
        "ESG Methodology": "This strategy integrates ESG criteria but may still be exposed to non-sustainable risks.",
        "Benchmark Comparison": "Benchmark returns are presented for comparison purposes only and do not reflect fees.",
        "Performance Past vs Future": "Historical returns are not a guarantee of future performance.",
        "Index Usage": "The strategy may invest in securities not included in the benchmark.",
        "Data Source": "Data used in ESG scoring is obtained from third-party sources deemed reliable.",
        "Carbon Footprint": "This fund's carbon footprint is calculated based on Scope 1 and 2 emissions.",
        "Proxy Voting": "All voting activity aligns with the firm's proxy voting policy and stewardship principles.",
        "Engagement Policy": "Company engagement is a key part of our ESG integration framework.",
        "Sustainable Investing Risk": "ESG ratings may not reflect all material sustainability risks.",
        "Regulatory Classification": "Distributed in accordance with local regulations governing UCITS.",
        "Holdings Disclosure": "Full details of underlying fund holdings can be found at www.ssga.com.",
        "Sector Allocation Methodology": "Sector classifications follow GICS methodology unless otherwise noted.",
        "Data Accuracy Disclaimer": "Information is not warranted to be accurate, complete, or timely.",
        "Third-Party Content Disclaimer": "Morningstar and its content providers disclaim liability for damages from use of the data.",
        "Marketing Classification": "This document is a marketing communication and not investment research.",
        "Jurisdictional Disclosure": "Distributed in accordance with Swiss and Luxembourg regulations.",
        "Investor Rights": "A summary of investor rights is available at www.ssga.com.",
        "Professional Use Only": "This communication is directed at professional clients only.",
        "Fund Documentation Reference": "Review the KID and Prospectus before making an investment decision.",
        "Source Attribution": "Source: Morningstar, Inc. and SSGA.",
        "Reproduction Restriction": "No part of this document may be reproduced without written consent."
    }

    disclosure_types = ['Legal', 'Regulatory', 'ESG Policy', 'Performance', 'Marketing']
    applies_to = ['Fund', 'Firm', 'Strategy', 'Share Class', 'All', 'Professional Clients']
    disclosure_sources = ['SSGA', 'Morningstar', 'Internal Compliance']

    data = []
    for i in range(num_records):
        topic = random.choice(topics)
        footnote = footnotes[topic]
        disclosure_id = f"DISC{i+100}"
        effective_date = fake.date_between(start_date='-3y', end_date='today')
        disclosure_type = random.choice(disclosure_types)
        applicable_entity = random.choice(applies_to)
        last_reviewed = fake.date_between(start_date=effective_date, end_date='today')
        reviewer = fake.name()
        disclosure_source = random.choice(disclosure_sources)

        # Assign disclosure requirement classification
        if disclosure_type in ['Legal', 'Regulatory']:
            requirement = 'Mandatory'
        else:
            requirement = random.choices(['Mandatory', 'Voluntary'], weights=[0.2, 0.8])[0]

        data.append({
            'DisclosureID': disclosure_id,
            'Topic': topic,
            'Footnote': footnote,
            'EffectiveDate': effective_date,
            'DisclosureType': disclosure_type,
            'AppliesTo': applicable_entity,
            'LastReviewedDate': last_reviewed,
            'ReviewedBy': reviewer,
            'DisclosureRequirement': requirement,
            'DisclosureSource': disclosure_source
        })

    df_disclosures = pd.DataFrame(data)
    df_disclosures.to_csv(output_path, index=False)
    print(f"Compliance data saved to: {output_path}")
    return df_disclosures

# Example usage
if __name__ == "__main__":
    df_disclosures = generate_full_compliance_data(50, 'full_compliance_footnotes.csv')

"""## **Load Compliance & Disclosure Footnotes**

This function loads the compliance footnotes CSV file into a pandas DataFrame, parses date fields (EffectiveDate and LastReviewedDate), and standardizes column names to lowercase for consistency. It also converts relevant columns—such as disclosuretype, appliesto, topic, and disclosurerequirement—into categorical types, which improves memory efficiency and prepares the data for structured analysis or dashboard integration.
"""

import pandas as pd

def load_compliance_data(file_path):
    """
    Loads and cleans the enhanced compliance footnotes and disclosures dataset.
    Parses dates and ensures proper data types for new fields.
    """
    # Load CSV and parse dates
    df_disclosures = pd.read_csv(file_path, parse_dates=['EffectiveDate', 'LastReviewedDate'])

    # Normalize column names
    df_disclosures.columns = df_disclosures.columns.str.strip().str.lower()

    # Convert appropriate columns to categorical
    categorical_fields = [
        'disclosuretype',
        'appliesto',
        'topic',
        'disclosurerequirement',
        'disclosuresource'
    ]
    for col in categorical_fields:
        if col in df_disclosures.columns:
            df_disclosures[col] = df_disclosures[col].astype('category')

    return df_disclosures

# Example usage
if __name__ == "__main__":
    df_disclosures = load_compliance_data('full_compliance_footnotes.csv')
    print(df_disclosures.head())

df_disclosures.info()
#there are systems dedicated to this (disclosure id and text and mandatory or not)