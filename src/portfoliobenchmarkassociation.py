# -*- coding: utf-8 -*-
"""PORTFOLIOBENCHMARKASSOCIATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ew7-9dVDN8kmoNpiD51tzXvX4XrvaM0z

# Portfolio-Benchmark Association Table Generator

This notebook generates a synthetic `PORTFOLIOBENCHMARKASSOCIATION` table that links ESG-themed investment portfolios to relevant benchmark indices. It is designed to simulate how portfolios are aligned with market or ESG benchmarks in a factsheet or performance analytics context.

The table includes:
- `PORTFOLIOCODE` — Identifier for each synthetic ESG portfolio
- `BENCHMARKCODE` — Associated benchmark ticker (e.g., ESGD, EFIV)
- `RANK` — A numeric value indicating the order of importance or relevance of each benchmark to the portfolio

This table is useful for:
- Factsheet generation
- Portfolio performance evaluation against benchmarks
- Feeding into downstream Snowflake tables for analytics or reporting

Each portfolio is randomly assigned 2–3 relevant benchmarks, and the rank field represents the hierarchy of those associations.

---

## Execution Instructions

1. Run the notebook from top to bottom.
2. The code will automatically generate benchmark associations for each portfolio.
3. The resulting DataFrame can be exported as a CSV or integrated into a larger data pipeline.

## Dependencies

- `pandas`
- `random` (built-in)

## Next Steps

- Export to Snowflake using your `load_to_snowflake()` utility or equivalent
- Extend logic to include benchmark metadata (e.g., benchmark names, categories)
- Validate consistency across benchmarked performance calculations
"""

import pandas as pd
import random

# Define portfolio codes (from previously created funds DataFrame)
portfolio_codes = [
    "Climate_Leaders_Fund",
    "Social_Impact_Fund",
    "Governance_Focused_Fund",
    "Low_Controversy_Fund",
    "Overall_ESG_Leaders",
    "Stakeholder_Advocacy_Fund"
]

# Define benchmark tickers
benchmark_tickers = ["'ENRG", "SHE", "VOTE", "ESGD", "EFIV"]

# Create synthetic PORTFOLIOBENCHMARKASSOCIATION table
benchmark_associations = []

for portfolio in portfolio_codes:
    # Randomly choose 2–3 benchmarks per portfolio
    assigned_benchmarks = random.sample(benchmark_tickers, k=random.randint(2, 3))

    for rank, benchmark in enumerate(assigned_benchmarks, start=1):
        benchmark_associations.append({
            "PORTFOLIOCODE": portfolio,       # PK
            "BENCHMARKCODE": benchmark,       # FK
            "RANK": rank                       # Order of relevance
        })

# Convert to DataFrame
df_portfolio_benchmark_association = pd.DataFrame(benchmark_associations)

# Preview
print(df_portfolio_benchmark_association.head())

"""##Potential Snowflake Implementation
This code is intended not only to demonstrate realistic data generation for academic or testing purposes but also to support end-to-end data pipelines where Snowflake acts as the target data warehouse. With minimal adjustments (e.g., schema renaming, batch control), this data can be used in:
* Report generation (e.g., Assette)
* Performance attribution systems
* ESG dashboards
* Fund analytics platforms

The code below shows a programmatic load using snowflake.connector in Python. Please see example.env to include your Snowflake database and API keys.
"""

import snowflake.connector

# Load credentials
load_dotenv()

sf_user = os.getenv("SNOWFLAKE_USER")
sf_password = os.getenv("SNOWFLAKE_PASSWORD")
sf_account = os.getenv("SNOWFLAKE_ACCOUNT")
sf_database = os.getenv("SNOWFLAKE_DATABASE")
sf_schema = os.getenv("SNOWFLAKE_SCHEMA")
sf_warehouse = os.getenv("SNOWFLAKE_WAREHOUSE")

# Connect to Snowflake
conn = snowflake.connector.connect(
    user=sf_user,
    password=sf_password,
    account=sf_account,
    warehouse=sf_warehouse,
    database=sf_database,
    schema=sf_schema
)

cursor = conn.cursor()

# Function to upload DataFrame
def append_to_snowflake(df, table_name):
    try:
        # Create temp CSV
        temp_csv = "/tmp/temp_fund_upload.csv"
        df.to_csv(temp_csv, index=False)

        # Create staging area in memory
        cursor.execute(f"PUT file://{temp_csv} @%{table_name} OVERWRITE = TRUE")

        # Copy from staged CSV to table
        columns = ",".join(df.columns)
        cursor.execute(f"""
            COPY INTO {table_name}
            FROM @%{table_name}
            FILE_FORMAT = (TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='"' SKIP_HEADER=1)
        """)

        print(f"✅ Data appended to {table_name} in Snowflake")

    except Exception as e:
        print("❌ Failed to upload data:", e)
    finally:
        cursor.close()
        conn.close()

# Example usage:
append_to_snowflake(df_portfolios, "PORTFOLIOGENERALINFORMATION")
