# -*- coding: utf-8 -*-
"""SECURITYMASTER - COUNTRY

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gy1ZmnMax1Hvzi5dsJwTytowVsIqlvaM
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

# Load Table 1
import pandas as pd
file_path = '/content/drive/MyDrive/country_names_codes.csv'

df = pd.read_csv(file_path, skiprows=3)
df

# Load Table 2
file_path2 = '/content/drive/MyDrive/region_codes.csv'

df2 = pd.read_csv(file_path2)
df2

# Merge these tables together
# Keep only necessary columns from currency table
currency_filtered = df[[
    "Country Name", "Country Code", "2023"
]].copy()

# Rename for clarity
currency_filtered.rename(columns={"2023": "LSU_PER_USD"}, inplace=True)

# Merge on Country Code to add region and income group
country_df = pd.merge(
    currency_filtered,
    df2[["Country Code", "Region", "IncomeGroup"]],
    on="Country Code",
    how="left"
)

country_df.rename(columns={
    "Country Name": "COUNTRY_NAME",
    "Country Code": "COUNTRY_CODE",
    "Indicator Code": "INDICATOR_CODE",
    "Region": "REGION",
    "IncomeGroup": "INCOME_GROUP"
}, inplace=True)

country_df

# Combine with Indicator Data
import pandas as pd
import requests
from functools import reduce
indicators = {
    "EG.FEC.RNEW.ZS": "Renewable Energy (% of Total)",
    "EG.ELC.COAL.ZS": "Coal Electricity (% of Total)",
    "SL.TLF.CACT.FE.ZS": "Female Labor Force Participation (%)",
    "IT.NET.USER.ZS": "Individuals using the Internet (% of population)",
    "GE.EST": "Gov. Effectiveness (Percentile)",
    "CC.EST": "Control of Corruption (Percentile)",
    "EN.ATM.PM25.MC.M3": "PM2.5 Pollution",
    "ER.PTD.TOTL.ZS": "Protected Land (%)",
    "SI.POV.DDAY": "Poverty Rate (%)",
    "SE.SEC.ENRR": "School Enrollment (%)",
    "RL.EST": "Rule of Law",
    "VA.EST": "Voice & Accountability"
}

# Step 1: Get actual countries (filter out aggregates and non-countries)
country_meta_url = "https://api.worldbank.org/v2/country?format=json&per_page=300"
response = requests.get(country_meta_url).json()

# Only real countries (not aggregates)
valid_countries = {
    c["id"]: c["name"]
    for c in response[1]
    if c["region"]["id"] != "NA" and c["id"].isalpha() and len(c["id"]) == 3
}

# Step 2: Pull ESG data per indicator for all countries
all_dfs = []

for code, label in indicators.items():
    print(f"Fetching: {label}")
    url = f"https://api.worldbank.org/v2/country/all/indicator/{code}?format=json&per_page=20000"
    data = requests.get(url).json()

    if len(data) > 1 and isinstance(data[1], list):
        df = pd.DataFrame(data[1])
        df = df[df["date"] == "2023"]
        df = df[["countryiso3code", "country", "value"]].copy()
        df.rename(columns={
            "countryiso3code": "Country Code",
            "country": "Country Name",
            "value": label
        }, inplace=True)
        df["Country Name"] = df["Country Name"].apply(lambda x: x["value"] if isinstance(x, dict) else x)
        all_dfs.append(df)

# Step 3: Merge all indicators together
def merge_dfs(left, right):
    return pd.merge(left, right, on=["Country Code", "Country Name"], how="outer")

indicator_df = reduce(merge_dfs, all_dfs)

# Step 4: Filter to only real countries
merged_df = indicator_df[indicator_df["Country Code"].isin(valid_countries.keys())].copy()

# Step 5: Round ESG values
indicator_df[list(indicators.values())] = indicator_df[list(indicators.values())].round(2)

# Normalize all ESG indicators to 0–100 scale
esg_cols = list(indicators.values())

for col in esg_cols:
    col_min = indicator_df[col].min(skipna=True)
    col_max = indicator_df[col].max(skipna=True)
    indicator_df[col] = ((indicator_df[col] - col_min) / (col_max - col_min)) * 100

# Compute ESG_Score as the average of normalized indicators
indicator_df["ESG_Score"] = indicator_df[esg_cols].mean(axis=1).round(2)

# Step 6: Reset index and display
indicator_df.reset_index(drop=True, inplace=True)
indicator_df.round(2)

# --- Step 1: List of ESG indicator columns ---
esg_cols = list(indicators.values())

# --- Step 2: Normalize each ESG column to 0–100 scale ---
for col in esg_cols:
    col_min = merged_df[col].min(skipna=True)
    col_max = merged_df[col].max(skipna=True)
    merged_df[col] = ((merged_df[col] - col_min) / (col_max - col_min)) * 100

# --- Step 3: Calculate ESG Score (mean of normalized values) ---
merged_df["ESG_Score"] = merged_df[esg_cols].mean(axis=1).round(2)

# --- Step 4: Keep only necessary columns ---
indicator_df = merged_df[["Country Name", "Country Code"] + esg_cols + ["ESG_Score"]].copy()
indicator_df[esg_cols + ["ESG_Score"]] = indicator_df[esg_cols + ["ESG_Score"]].round(2)

# --- Step 6: Display shape and preview ---
print(f"Final indicator DataFrame shape: {indicator_df.shape}")
indicator_df.head()

# Merge the important columns of the 2 tables together
# Merge on Country Code and Name
COUNTRY = pd.merge(
    country_df,
    indicator_df,
    how="left",
    left_on=["COUNTRY_CODE", "COUNTRY_NAME"],
    right_on=["Country Code", "Country Name"]
)

# Drop redundant columns
COUNTRY.drop(columns=["Country Code", "Country Name"], inplace=True)

# Standardize all column names to UPPER_CASE_WITH_UNDERSCORES
COUNTRY.columns = [
    col.upper().replace(" ", "_").replace("(", "").replace(")", "").replace("%", "PERCENT").replace("/", "_PER_")
    for col in COUNTRY.columns
]
COUNTRY = COUNTRY.round(2)
COUNTRY

!pip install snowflake-connector-python python-dotenv

#Load the .env file
from google.colab import files
files.upload()

import os
if os.path.exists("env.txt"):
    os.rename("env.txt", ".env")
    print("Renamed env.txt to .env")
else:
    print("env.txt not found")

from dotenv import load_dotenv
load_dotenv()
print(".env loaded")

import snowflake.connector
#Basic test connection
conn = snowflake.connector.connect(
    user=os.getenv("SNOWFLAKE_USER"),
    password=os.getenv("SNOWFLAKE_PASSWORD"),
    account=os.getenv("SNOWFLAKE_ACCOUNT")
)

print("Basic connection to Snowflake succeeded")
conn.close()

#Use .env paramaters to connect to snowflake
def get_snowflake_connection():
    return snowflake.connector.connect(
        user=os.getenv("SNOWFLAKE_USER"),
        password=os.getenv("SNOWFLAKE_PASSWORD"),
        account=os.getenv("SNOWFLAKE_ACCOUNT"),
        role=os.getenv("SNOWFLAKE_ROLE"),
        warehouse=os.getenv("SNOWFLAKE_WAREHOUSE"),
        database=os.getenv("SNOWFLAKE_DATABASE"),
        schema=os.getenv("SNOWFLAKE_SCHEMA")
    )
#Connection - connection is authenticated
connection = get_snowflake_connection()
#Lets me run SQL commands
cursor = connection.cursor()

from snowflake.connector.pandas_tools import write_pandas

def safe_quote(col: str) -> str:
    """
    Ensures column names are safely quoted for Snowflake SQL syntax.
    Replaces internal quotes and wraps the name in double quotes.
    """
    col = str(col).replace('"', '""').strip()
    return f'"{col}"'

def map_dtype_to_snowflake(dtype):
    """
    Maps pandas dtypes to Snowflake SQL data types.
    """
    if pd.api.types.is_float_dtype(dtype):
        return "FLOAT"
    elif pd.api.types.is_integer_dtype(dtype):
        return "NUMBER"
    else:
        return "VARCHAR"

def load_to_snowflake_merge(df, table_name, conn, unique_keys):
    """
    Uploads DataFrame to Snowflake with type inference and merge logic.
    """
    cur = conn.cursor()
    df_cols = df.columns.tolist()
    temp_table = f"{table_name}_STAGING"

    # Step 1: Infer column types and create table if needed
    col_defs = ", ".join([
        f"{safe_quote(col)} {map_dtype_to_snowflake(df[col].dtype)}"
        for col in df_cols
    ])
    cur.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({col_defs})")

    # Step 2: Add any missing columns to the main table
    cur.execute(f"DESC TABLE {table_name}")
    existing_cols = {row[0].upper() for row in cur.fetchall()}
    for col in df_cols:
        if col.upper() not in existing_cols:
            col_type = map_dtype_to_snowflake(df[col].dtype)
            cur.execute(f"ALTER TABLE {table_name} ADD COLUMN {safe_quote(col)} {col_type}")

    # Step 3: Create staging table
    cur.execute(f"CREATE OR REPLACE TABLE {temp_table} ({col_defs})")
    write_pandas(conn, df, temp_table)

    # Step 4: Merge without duplication
    on_clause = " AND ".join([f"t.{safe_quote(col)} = s.{safe_quote(col)}" for col in unique_keys])
    insert_cols = ", ".join([safe_quote(col) for col in df_cols])
    insert_vals = ", ".join([f"s.{safe_quote(col)}" for col in df_cols])

    merge_stmt = f"""
        MERGE INTO {table_name} t
        USING {temp_table} s
        ON {on_clause}
        WHEN NOT MATCHED THEN
            INSERT ({insert_cols}) VALUES ({insert_vals})
    """
    cur.execute(merge_stmt)

    # Step 5: Clean up
    cur.execute(f"DROP TABLE IF EXISTS {temp_table}")
    cur.close()
    conn.close()

    print(f"{table_name} updated. Duplicates prevented using keys: {unique_keys}")

#Test the connection
cursor.execute("SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE(), CURRENT_DATE;")

for row in cursor:
    print(row)

# Step 1: Reconnect to Snowflake
connection = get_snowflake_connection()

# Step 2: Define table name and primary keys
country_table_name = "COUNTRY"
unique_keys = ["COUNTRY_CODE"]  # prevent duplicates per country/year

# Step 3: Upload to Snowflake
load_to_snowflake_merge(COUNTRY, country_table_name, connection, unique_keys)

COUNTRY.info()

# Query the data from Snowflake
query = "SELECT * FROM COUNTRY LIMIT 10"
cursor.execute(query)

# Fetch the results
results = cursor.fetchall()

# Print the results
for row in results:
    print(row)

